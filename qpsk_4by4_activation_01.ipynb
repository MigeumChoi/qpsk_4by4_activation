{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUhH1gFREwcHQK+56XvYCd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bkVK85rQgpKD","executionInfo":{"status":"ok","timestamp":1695615250247,"user_tz":-540,"elapsed":42440,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"65b84f98-8350-425b-ce9b-2771f456eed9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3666\n","Epoch 1: val_loss improved from inf to 0.34909, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 900ms/step - loss: 0.3666 - val_loss: 0.3491\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3626\n","Epoch 2: val_loss improved from 0.34909 to 0.34544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.3626 - val_loss: 0.3454\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3587\n","Epoch 3: val_loss improved from 0.34544 to 0.34185, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3587 - val_loss: 0.3418\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3549\n","Epoch 4: val_loss improved from 0.34185 to 0.33834, saving model to hl5_0100.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 51ms/step - loss: 0.3549 - val_loss: 0.3383\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3511\n","Epoch 5: val_loss improved from 0.33834 to 0.33491, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3511 - val_loss: 0.3349\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3475\n","Epoch 6: val_loss improved from 0.33491 to 0.33154, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3475 - val_loss: 0.3315\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3439\n","Epoch 7: val_loss improved from 0.33154 to 0.32822, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.3439 - val_loss: 0.3282\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3403\n","Epoch 8: val_loss improved from 0.32822 to 0.32496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.3403 - val_loss: 0.3250\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3368\n","Epoch 9: val_loss improved from 0.32496 to 0.32174, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3368 - val_loss: 0.3217\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3334\n","Epoch 10: val_loss improved from 0.32174 to 0.31857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3334 - val_loss: 0.3186\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3300\n","Epoch 11: val_loss improved from 0.31857 to 0.31544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3300 - val_loss: 0.3154\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3266\n","Epoch 12: val_loss improved from 0.31544 to 0.31234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3266 - val_loss: 0.3123\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3233\n","Epoch 13: val_loss improved from 0.31234 to 0.30929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.3233 - val_loss: 0.3093\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3201\n","Epoch 14: val_loss improved from 0.30929 to 0.30627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3201 - val_loss: 0.3063\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3168\n","Epoch 15: val_loss improved from 0.30627 to 0.30329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3168 - val_loss: 0.3033\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3136\n","Epoch 16: val_loss improved from 0.30329 to 0.30035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.3136 - val_loss: 0.3004\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3105\n","Epoch 17: val_loss improved from 0.30035 to 0.29744, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3105 - val_loss: 0.2974\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3074\n","Epoch 18: val_loss improved from 0.29744 to 0.29457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3074 - val_loss: 0.2946\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3043\n","Epoch 19: val_loss improved from 0.29457 to 0.29172, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3043 - val_loss: 0.2917\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3012\n","Epoch 20: val_loss improved from 0.29172 to 0.28891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3012 - val_loss: 0.2889\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2982\n","Epoch 21: val_loss improved from 0.28891 to 0.28613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2982 - val_loss: 0.2861\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2952\n","Epoch 22: val_loss improved from 0.28613 to 0.28339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2952 - val_loss: 0.2834\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2923\n","Epoch 23: val_loss improved from 0.28339 to 0.28067, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2923 - val_loss: 0.2807\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2894\n","Epoch 24: val_loss improved from 0.28067 to 0.27799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2894 - val_loss: 0.2780\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2865\n","Epoch 25: val_loss improved from 0.27799 to 0.27533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2865 - val_loss: 0.2753\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2836\n","Epoch 26: val_loss improved from 0.27533 to 0.27270, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2836 - val_loss: 0.2727\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2808\n","Epoch 27: val_loss improved from 0.27270 to 0.27011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2808 - val_loss: 0.2701\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2780\n","Epoch 28: val_loss improved from 0.27011 to 0.26754, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2780 - val_loss: 0.2675\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2752\n","Epoch 29: val_loss improved from 0.26754 to 0.26500, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2752 - val_loss: 0.2650\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2725\n","Epoch 30: val_loss improved from 0.26500 to 0.26249, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2725 - val_loss: 0.2625\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2698\n","Epoch 31: val_loss improved from 0.26249 to 0.26000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.2698 - val_loss: 0.2600\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2671\n","Epoch 32: val_loss improved from 0.26000 to 0.25755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2671 - val_loss: 0.2575\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2644\n","Epoch 33: val_loss improved from 0.25755 to 0.25512, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2644 - val_loss: 0.2551\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2618\n","Epoch 34: val_loss improved from 0.25512 to 0.25271, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2618 - val_loss: 0.2527\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2592\n","Epoch 35: val_loss improved from 0.25271 to 0.25034, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2592 - val_loss: 0.2503\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2566\n","Epoch 36: val_loss improved from 0.25034 to 0.24798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.2566 - val_loss: 0.2480\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2541\n","Epoch 37: val_loss improved from 0.24798 to 0.24566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2541 - val_loss: 0.2457\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2516\n","Epoch 38: val_loss improved from 0.24566 to 0.24336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2516 - val_loss: 0.2434\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2491\n","Epoch 39: val_loss improved from 0.24336 to 0.24109, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2491 - val_loss: 0.2411\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2466\n","Epoch 40: val_loss improved from 0.24109 to 0.23884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2466 - val_loss: 0.2388\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2442\n","Epoch 41: val_loss improved from 0.23884 to 0.23661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2442 - val_loss: 0.2366\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2418\n","Epoch 42: val_loss improved from 0.23661 to 0.23441, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2418 - val_loss: 0.2344\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2394\n","Epoch 43: val_loss improved from 0.23441 to 0.23224, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2394 - val_loss: 0.2322\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2370\n","Epoch 44: val_loss improved from 0.23224 to 0.23009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2370 - val_loss: 0.2301\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2347\n","Epoch 45: val_loss improved from 0.23009 to 0.22796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2347 - val_loss: 0.2280\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2324\n","Epoch 46: val_loss improved from 0.22796 to 0.22586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2324 - val_loss: 0.2259\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2301\n","Epoch 47: val_loss improved from 0.22586 to 0.22378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2301 - val_loss: 0.2238\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2278\n","Epoch 48: val_loss improved from 0.22378 to 0.22173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2278 - val_loss: 0.2217\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2256\n","Epoch 49: val_loss improved from 0.22173 to 0.21970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2256 - val_loss: 0.2197\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2234\n","Epoch 50: val_loss improved from 0.21970 to 0.21769, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2234 - val_loss: 0.2177\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2212\n","Epoch 51: val_loss improved from 0.21769 to 0.21570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.2212 - val_loss: 0.2157\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2190\n","Epoch 52: val_loss improved from 0.21570 to 0.21373, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2190 - val_loss: 0.2137\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2169\n","Epoch 53: val_loss improved from 0.21373 to 0.21179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2169 - val_loss: 0.2118\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2148\n","Epoch 54: val_loss improved from 0.21179 to 0.20987, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2148 - val_loss: 0.2099\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2127\n","Epoch 55: val_loss improved from 0.20987 to 0.20797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2127 - val_loss: 0.2080\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2106\n","Epoch 56: val_loss improved from 0.20797 to 0.20609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2106 - val_loss: 0.2061\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 57: val_loss improved from 0.20609 to 0.20423, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2085 - val_loss: 0.2042\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2065\n","Epoch 58: val_loss improved from 0.20423 to 0.20239, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2065 - val_loss: 0.2024\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2045\n","Epoch 59: val_loss improved from 0.20239 to 0.20058, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2045 - val_loss: 0.2006\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2025\n","Epoch 60: val_loss improved from 0.20058 to 0.19878, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2025 - val_loss: 0.1988\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2005\n","Epoch 61: val_loss improved from 0.19878 to 0.19700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2005 - val_loss: 0.1970\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1986\n","Epoch 62: val_loss improved from 0.19700 to 0.19525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1986 - val_loss: 0.1952\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1966\n","Epoch 63: val_loss improved from 0.19525 to 0.19351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1966 - val_loss: 0.1935\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1947\n","Epoch 64: val_loss improved from 0.19351 to 0.19179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1947 - val_loss: 0.1918\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1928\n","Epoch 65: val_loss improved from 0.19179 to 0.19009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1928 - val_loss: 0.1901\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1910\n","Epoch 66: val_loss improved from 0.19009 to 0.18841, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1910 - val_loss: 0.1884\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 67: val_loss improved from 0.18841 to 0.18675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1891 - val_loss: 0.1867\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1873\n","Epoch 68: val_loss improved from 0.18675 to 0.18510, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1873 - val_loss: 0.1851\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1855\n","Epoch 69: val_loss improved from 0.18510 to 0.18348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1855 - val_loss: 0.1835\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1837\n","Epoch 70: val_loss improved from 0.18348 to 0.18187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1837 - val_loss: 0.1819\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1819\n","Epoch 71: val_loss improved from 0.18187 to 0.18028, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1819 - val_loss: 0.1803\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1802\n","Epoch 72: val_loss improved from 0.18028 to 0.17871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1802 - val_loss: 0.1787\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1784\n","Epoch 73: val_loss improved from 0.17871 to 0.17715, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1784 - val_loss: 0.1772\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1767\n","Epoch 74: val_loss improved from 0.17715 to 0.17562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1767 - val_loss: 0.1756\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1750\n","Epoch 75: val_loss improved from 0.17562 to 0.17410, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1750 - val_loss: 0.1741\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1734\n","Epoch 76: val_loss improved from 0.17410 to 0.17260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1734 - val_loss: 0.1726\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1717\n","Epoch 77: val_loss improved from 0.17260 to 0.17111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1717 - val_loss: 0.1711\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1701\n","Epoch 78: val_loss improved from 0.17111 to 0.16964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1701 - val_loss: 0.1696\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1684\n","Epoch 79: val_loss improved from 0.16964 to 0.16819, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1684 - val_loss: 0.1682\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1668\n","Epoch 80: val_loss improved from 0.16819 to 0.16675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1668 - val_loss: 0.1668\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1652\n","Epoch 81: val_loss improved from 0.16675 to 0.16534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1652 - val_loss: 0.1653\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1637\n","Epoch 82: val_loss improved from 0.16534 to 0.16393, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1637 - val_loss: 0.1639\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1621\n","Epoch 83: val_loss improved from 0.16393 to 0.16255, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1621 - val_loss: 0.1625\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1606\n","Epoch 84: val_loss improved from 0.16255 to 0.16118, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1606 - val_loss: 0.1612\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1590\n","Epoch 85: val_loss improved from 0.16118 to 0.15982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1590 - val_loss: 0.1598\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1575\n","Epoch 86: val_loss improved from 0.15982 to 0.15848, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1575 - val_loss: 0.1585\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1560\n","Epoch 87: val_loss improved from 0.15848 to 0.15716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1560 - val_loss: 0.1572\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1546\n","Epoch 88: val_loss improved from 0.15716 to 0.15585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1546 - val_loss: 0.1558\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1531\n","Epoch 89: val_loss improved from 0.15585 to 0.15456, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1531 - val_loss: 0.1546\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1517\n","Epoch 90: val_loss improved from 0.15456 to 0.15328, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1517 - val_loss: 0.1533\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1502\n","Epoch 91: val_loss improved from 0.15328 to 0.15201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1502 - val_loss: 0.1520\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1488\n","Epoch 92: val_loss improved from 0.15201 to 0.15076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1488 - val_loss: 0.1508\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1474\n","Epoch 93: val_loss improved from 0.15076 to 0.14953, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1474 - val_loss: 0.1495\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1461\n","Epoch 94: val_loss improved from 0.14953 to 0.14831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.1461 - val_loss: 0.1483\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1447\n","Epoch 95: val_loss improved from 0.14831 to 0.14710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1447 - val_loss: 0.1471\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1433\n","Epoch 96: val_loss improved from 0.14710 to 0.14591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1433 - val_loss: 0.1459\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1420\n","Epoch 97: val_loss improved from 0.14591 to 0.14473, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1420 - val_loss: 0.1447\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1407\n","Epoch 98: val_loss improved from 0.14473 to 0.14356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.1407 - val_loss: 0.1436\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1394\n","Epoch 99: val_loss improved from 0.14356 to 0.14241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1394 - val_loss: 0.1424\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1381\n","Epoch 100: val_loss improved from 0.14241 to 0.14127, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.1381 - val_loss: 0.1413\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1368\n","Epoch 101: val_loss improved from 0.14127 to 0.14014, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1368 - val_loss: 0.1401\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1355\n","Epoch 102: val_loss improved from 0.14014 to 0.13903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1355 - val_loss: 0.1390\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1343\n","Epoch 103: val_loss improved from 0.13903 to 0.13793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1343 - val_loss: 0.1379\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1331\n","Epoch 104: val_loss improved from 0.13793 to 0.13685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1331 - val_loss: 0.1368\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1318\n","Epoch 105: val_loss improved from 0.13685 to 0.13577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1318 - val_loss: 0.1358\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1306\n","Epoch 106: val_loss improved from 0.13577 to 0.13471, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1306 - val_loss: 0.1347\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1294\n","Epoch 107: val_loss improved from 0.13471 to 0.13366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1294 - val_loss: 0.1337\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1282\n","Epoch 108: val_loss improved from 0.13366 to 0.13262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1282 - val_loss: 0.1326\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1271\n","Epoch 109: val_loss improved from 0.13262 to 0.13160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.1271 - val_loss: 0.1316\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1259\n","Epoch 110: val_loss improved from 0.13160 to 0.13059, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1259 - val_loss: 0.1306\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1248\n","Epoch 111: val_loss improved from 0.13059 to 0.12959, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1248 - val_loss: 0.1296\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1236\n","Epoch 112: val_loss improved from 0.12959 to 0.12860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1236 - val_loss: 0.1286\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1225\n","Epoch 113: val_loss improved from 0.12860 to 0.12762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1225 - val_loss: 0.1276\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1214\n","Epoch 114: val_loss improved from 0.12762 to 0.12665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1214 - val_loss: 0.1267\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1203\n","Epoch 115: val_loss improved from 0.12665 to 0.12570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1203 - val_loss: 0.1257\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1192\n","Epoch 116: val_loss improved from 0.12570 to 0.12476, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1192 - val_loss: 0.1248\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1181\n","Epoch 117: val_loss improved from 0.12476 to 0.12382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1181 - val_loss: 0.1238\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1171\n","Epoch 118: val_loss improved from 0.12382 to 0.12290, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1171 - val_loss: 0.1229\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1160\n","Epoch 119: val_loss improved from 0.12290 to 0.12199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1160 - val_loss: 0.1220\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1150\n","Epoch 120: val_loss improved from 0.12199 to 0.12109, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1150 - val_loss: 0.1211\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1140\n","Epoch 121: val_loss improved from 0.12109 to 0.12020, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1140 - val_loss: 0.1202\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1130\n","Epoch 122: val_loss improved from 0.12020 to 0.11933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1130 - val_loss: 0.1193\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1120\n","Epoch 123: val_loss improved from 0.11933 to 0.11846, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1120 - val_loss: 0.1185\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1110\n","Epoch 124: val_loss improved from 0.11846 to 0.11760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1110 - val_loss: 0.1176\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1100\n","Epoch 125: val_loss improved from 0.11760 to 0.11675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1100 - val_loss: 0.1168\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1090\n","Epoch 126: val_loss improved from 0.11675 to 0.11592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1090 - val_loss: 0.1159\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1081\n","Epoch 127: val_loss improved from 0.11592 to 0.11509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1081 - val_loss: 0.1151\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 128: val_loss improved from 0.11509 to 0.11427, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1071 - val_loss: 0.1143\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1062\n","Epoch 129: val_loss improved from 0.11427 to 0.11347, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1062 - val_loss: 0.1135\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1052\n","Epoch 130: val_loss improved from 0.11347 to 0.11267, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1052 - val_loss: 0.1127\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1043\n","Epoch 131: val_loss improved from 0.11267 to 0.11188, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1043 - val_loss: 0.1119\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1034\n","Epoch 132: val_loss improved from 0.11188 to 0.11110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1034 - val_loss: 0.1111\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1025\n","Epoch 133: val_loss improved from 0.11110 to 0.11033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1025 - val_loss: 0.1103\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1016\n","Epoch 134: val_loss improved from 0.11033 to 0.10957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1016 - val_loss: 0.1096\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1008\n","Epoch 135: val_loss improved from 0.10957 to 0.10882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1008 - val_loss: 0.1088\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0999\n","Epoch 136: val_loss improved from 0.10882 to 0.10808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0999 - val_loss: 0.1081\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0990\n","Epoch 137: val_loss improved from 0.10808 to 0.10735, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0990 - val_loss: 0.1073\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0982\n","Epoch 138: val_loss improved from 0.10735 to 0.10662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0982 - val_loss: 0.1066\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0973\n","Epoch 139: val_loss improved from 0.10662 to 0.10591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0973 - val_loss: 0.1059\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0965\n","Epoch 140: val_loss improved from 0.10591 to 0.10520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0965 - val_loss: 0.1052\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0957\n","Epoch 141: val_loss improved from 0.10520 to 0.10450, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0957 - val_loss: 0.1045\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0949\n","Epoch 142: val_loss improved from 0.10450 to 0.10381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0949 - val_loss: 0.1038\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0941\n","Epoch 143: val_loss improved from 0.10381 to 0.10313, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0941 - val_loss: 0.1031\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0933\n","Epoch 144: val_loss improved from 0.10313 to 0.10246, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0933 - val_loss: 0.1025\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0925\n","Epoch 145: val_loss improved from 0.10246 to 0.10180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0925 - val_loss: 0.1018\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0917\n","Epoch 146: val_loss improved from 0.10180 to 0.10114, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0917 - val_loss: 0.1011\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0910\n","Epoch 147: val_loss improved from 0.10114 to 0.10049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0910 - val_loss: 0.1005\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0902\n","Epoch 148: val_loss improved from 0.10049 to 0.09985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0902 - val_loss: 0.0999\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0895\n","Epoch 149: val_loss improved from 0.09985 to 0.09922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0895 - val_loss: 0.0992\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0887\n","Epoch 150: val_loss improved from 0.09922 to 0.09860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0887 - val_loss: 0.0986\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0880\n","Epoch 151: val_loss improved from 0.09860 to 0.09798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0880 - val_loss: 0.0980\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0873\n","Epoch 152: val_loss improved from 0.09798 to 0.09737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0873 - val_loss: 0.0974\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0865\n","Epoch 153: val_loss improved from 0.09737 to 0.09677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0865 - val_loss: 0.0968\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0858\n","Epoch 154: val_loss improved from 0.09677 to 0.09617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0858 - val_loss: 0.0962\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0851\n","Epoch 155: val_loss improved from 0.09617 to 0.09559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0851 - val_loss: 0.0956\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0844\n","Epoch 156: val_loss improved from 0.09559 to 0.09501, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0844 - val_loss: 0.0950\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0838\n","Epoch 157: val_loss improved from 0.09501 to 0.09444, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0838 - val_loss: 0.0944\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0831\n","Epoch 158: val_loss improved from 0.09444 to 0.09387, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0831 - val_loss: 0.0939\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0824\n","Epoch 159: val_loss improved from 0.09387 to 0.09331, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0824 - val_loss: 0.0933\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0818\n","Epoch 160: val_loss improved from 0.09331 to 0.09276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0818 - val_loss: 0.0928\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0811\n","Epoch 161: val_loss improved from 0.09276 to 0.09222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0811 - val_loss: 0.0922\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0805\n","Epoch 162: val_loss improved from 0.09222 to 0.09168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0805 - val_loss: 0.0917\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0798\n","Epoch 163: val_loss improved from 0.09168 to 0.09115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0798 - val_loss: 0.0912\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0792\n","Epoch 164: val_loss improved from 0.09115 to 0.09063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0792 - val_loss: 0.0906\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0786\n","Epoch 165: val_loss improved from 0.09063 to 0.09011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0786 - val_loss: 0.0901\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0779\n","Epoch 166: val_loss improved from 0.09011 to 0.08960, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0779 - val_loss: 0.0896\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0773\n","Epoch 167: val_loss improved from 0.08960 to 0.08910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0773 - val_loss: 0.0891\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0767\n","Epoch 168: val_loss improved from 0.08910 to 0.08860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0767 - val_loss: 0.0886\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 169: val_loss improved from 0.08860 to 0.08811, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0761 - val_loss: 0.0881\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 170: val_loss improved from 0.08811 to 0.08762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0755 - val_loss: 0.0876\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0750\n","Epoch 171: val_loss improved from 0.08762 to 0.08714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0750 - val_loss: 0.0871\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 172: val_loss improved from 0.08714 to 0.08667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0744 - val_loss: 0.0867\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0738\n","Epoch 173: val_loss improved from 0.08667 to 0.08620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0738 - val_loss: 0.0862\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0733\n","Epoch 174: val_loss improved from 0.08620 to 0.08574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0733 - val_loss: 0.0857\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 175: val_loss improved from 0.08574 to 0.08529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0727 - val_loss: 0.0853\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0722\n","Epoch 176: val_loss improved from 0.08529 to 0.08484, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0722 - val_loss: 0.0848\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0716\n","Epoch 177: val_loss improved from 0.08484 to 0.08440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0716 - val_loss: 0.0844\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0711\n","Epoch 178: val_loss improved from 0.08440 to 0.08396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0711 - val_loss: 0.0840\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0705\n","Epoch 179: val_loss improved from 0.08396 to 0.08353, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0705 - val_loss: 0.0835\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0700\n","Epoch 180: val_loss improved from 0.08353 to 0.08310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0700 - val_loss: 0.0831\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0695\n","Epoch 181: val_loss improved from 0.08310 to 0.08268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0695 - val_loss: 0.0827\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0690\n","Epoch 182: val_loss improved from 0.08268 to 0.08227, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0690 - val_loss: 0.0823\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0685\n","Epoch 183: val_loss improved from 0.08227 to 0.08186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0685 - val_loss: 0.0819\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0680\n","Epoch 184: val_loss improved from 0.08186 to 0.08145, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0680 - val_loss: 0.0815\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0675\n","Epoch 185: val_loss improved from 0.08145 to 0.08105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0675 - val_loss: 0.0811\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0670\n","Epoch 186: val_loss improved from 0.08105 to 0.08066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0670 - val_loss: 0.0807\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0665\n","Epoch 187: val_loss improved from 0.08066 to 0.08027, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0665 - val_loss: 0.0803\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0660\n","Epoch 188: val_loss improved from 0.08027 to 0.07989, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0660 - val_loss: 0.0799\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0656\n","Epoch 189: val_loss improved from 0.07989 to 0.07951, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0656 - val_loss: 0.0795\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 190: val_loss improved from 0.07951 to 0.07913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0651 - val_loss: 0.0791\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0646\n","Epoch 191: val_loss improved from 0.07913 to 0.07876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0646 - val_loss: 0.0788\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0642\n","Epoch 192: val_loss improved from 0.07876 to 0.07840, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0642 - val_loss: 0.0784\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0637\n","Epoch 193: val_loss improved from 0.07840 to 0.07804, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0637 - val_loss: 0.0780\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0633\n","Epoch 194: val_loss improved from 0.07804 to 0.07769, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0633 - val_loss: 0.0777\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 195: val_loss improved from 0.07769 to 0.07734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0629 - val_loss: 0.0773\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0624\n","Epoch 196: val_loss improved from 0.07734 to 0.07699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0624 - val_loss: 0.0770\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0620\n","Epoch 197: val_loss improved from 0.07699 to 0.07665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0620 - val_loss: 0.0767\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 198: val_loss improved from 0.07665 to 0.07632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0616 - val_loss: 0.0763\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0611\n","Epoch 199: val_loss improved from 0.07632 to 0.07599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0611 - val_loss: 0.0760\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0607\n","Epoch 200: val_loss improved from 0.07599 to 0.07566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0607 - val_loss: 0.0757\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0603\n","Epoch 201: val_loss improved from 0.07566 to 0.07534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0603 - val_loss: 0.0753\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 202: val_loss improved from 0.07534 to 0.07502, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0599 - val_loss: 0.0750\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0595\n","Epoch 203: val_loss improved from 0.07502 to 0.07471, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0595 - val_loss: 0.0747\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0591\n","Epoch 204: val_loss improved from 0.07471 to 0.07440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0591 - val_loss: 0.0744\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0587\n","Epoch 205: val_loss improved from 0.07440 to 0.07409, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0587 - val_loss: 0.0741\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0583\n","Epoch 206: val_loss improved from 0.07409 to 0.07379, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0583 - val_loss: 0.0738\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0580\n","Epoch 207: val_loss improved from 0.07379 to 0.07349, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0580 - val_loss: 0.0735\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 208: val_loss improved from 0.07349 to 0.07320, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0576 - val_loss: 0.0732\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0572\n","Epoch 209: val_loss improved from 0.07320 to 0.07291, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0572 - val_loss: 0.0729\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 210: val_loss improved from 0.07291 to 0.07263, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0569 - val_loss: 0.0726\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 211: val_loss improved from 0.07263 to 0.07234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0565 - val_loss: 0.0723\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0561\n","Epoch 212: val_loss improved from 0.07234 to 0.07207, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0561 - val_loss: 0.0721\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 213: val_loss improved from 0.07207 to 0.07179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0558 - val_loss: 0.0718\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 214: val_loss improved from 0.07179 to 0.07152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0554 - val_loss: 0.0715\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 215: val_loss improved from 0.07152 to 0.07126, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0551 - val_loss: 0.0713\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 216: val_loss improved from 0.07126 to 0.07099, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0547 - val_loss: 0.0710\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0544\n","Epoch 217: val_loss improved from 0.07099 to 0.07074, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0544 - val_loss: 0.0707\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0541\n","Epoch 218: val_loss improved from 0.07074 to 0.07048, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0541 - val_loss: 0.0705\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 219: val_loss improved from 0.07048 to 0.07023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0537 - val_loss: 0.0702\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 220: val_loss improved from 0.07023 to 0.06998, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0534 - val_loss: 0.0700\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0531\n","Epoch 221: val_loss improved from 0.06998 to 0.06974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0531 - val_loss: 0.0697\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0528\n","Epoch 222: val_loss improved from 0.06974 to 0.06950, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0528 - val_loss: 0.0695\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0525\n","Epoch 223: val_loss improved from 0.06950 to 0.06926, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0525 - val_loss: 0.0693\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 224: val_loss improved from 0.06926 to 0.06902, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0521 - val_loss: 0.0690\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 225: val_loss improved from 0.06902 to 0.06879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0518 - val_loss: 0.0688\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 226: val_loss improved from 0.06879 to 0.06856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0515 - val_loss: 0.0686\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 227: val_loss improved from 0.06856 to 0.06834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0512 - val_loss: 0.0683\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 228: val_loss improved from 0.06834 to 0.06812, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0509 - val_loss: 0.0681\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0507\n","Epoch 229: val_loss improved from 0.06812 to 0.06790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0507 - val_loss: 0.0679\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 230: val_loss improved from 0.06790 to 0.06768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0504 - val_loss: 0.0677\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0501\n","Epoch 231: val_loss improved from 0.06768 to 0.06747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0501 - val_loss: 0.0675\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 232: val_loss improved from 0.06747 to 0.06726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0498 - val_loss: 0.0673\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 233: val_loss improved from 0.06726 to 0.06706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0495 - val_loss: 0.0671\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 234: val_loss improved from 0.06706 to 0.06685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0492 - val_loss: 0.0669\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 235: val_loss improved from 0.06685 to 0.06665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0490 - val_loss: 0.0667\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 236: val_loss improved from 0.06665 to 0.06646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0487 - val_loss: 0.0665\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 237: val_loss improved from 0.06646 to 0.06626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0484 - val_loss: 0.0663\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 238: val_loss improved from 0.06626 to 0.06607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0482 - val_loss: 0.0661\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 239: val_loss improved from 0.06607 to 0.06588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0479 - val_loss: 0.0659\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 240: val_loss improved from 0.06588 to 0.06569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0477 - val_loss: 0.0657\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 241: val_loss improved from 0.06569 to 0.06551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0474 - val_loss: 0.0655\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 242: val_loss improved from 0.06551 to 0.06533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0472 - val_loss: 0.0653\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 243: val_loss improved from 0.06533 to 0.06515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0469 - val_loss: 0.0652\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 244: val_loss improved from 0.06515 to 0.06498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0467 - val_loss: 0.0650\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0464\n","Epoch 245: val_loss improved from 0.06498 to 0.06480, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0464 - val_loss: 0.0648\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 246: val_loss improved from 0.06480 to 0.06463, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0462 - val_loss: 0.0646\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 247: val_loss improved from 0.06463 to 0.06446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0460 - val_loss: 0.0645\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 248: val_loss improved from 0.06446 to 0.06430, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0457 - val_loss: 0.0643\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 249: val_loss improved from 0.06430 to 0.06414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0455 - val_loss: 0.0641\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0453\n","Epoch 250: val_loss improved from 0.06414 to 0.06398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0453 - val_loss: 0.0640\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 251: val_loss improved from 0.06398 to 0.06382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0451 - val_loss: 0.0638\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 252: val_loss improved from 0.06382 to 0.06366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0448 - val_loss: 0.0637\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 253: val_loss improved from 0.06366 to 0.06351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0446 - val_loss: 0.0635\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 254: val_loss improved from 0.06351 to 0.06336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0444 - val_loss: 0.0634\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 255: val_loss improved from 0.06336 to 0.06321, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0442 - val_loss: 0.0632\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 256: val_loss improved from 0.06321 to 0.06306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0440 - val_loss: 0.0631\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 257: val_loss improved from 0.06306 to 0.06292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0438 - val_loss: 0.0629\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 258: val_loss improved from 0.06292 to 0.06277, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0436 - val_loss: 0.0628\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 259: val_loss improved from 0.06277 to 0.06263, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0434 - val_loss: 0.0626\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 260: val_loss improved from 0.06263 to 0.06250, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0432 - val_loss: 0.0625\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 261: val_loss improved from 0.06250 to 0.06236, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0430 - val_loss: 0.0624\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 262: val_loss improved from 0.06236 to 0.06223, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0428 - val_loss: 0.0622\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 263: val_loss improved from 0.06223 to 0.06209, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0426 - val_loss: 0.0621\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 264: val_loss improved from 0.06209 to 0.06197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0424 - val_loss: 0.0620\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 265: val_loss improved from 0.06197 to 0.06184, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0422 - val_loss: 0.0618\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 266: val_loss improved from 0.06184 to 0.06171, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0420 - val_loss: 0.0617\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 267: val_loss improved from 0.06171 to 0.06159, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 268: val_loss improved from 0.06159 to 0.06147, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0417 - val_loss: 0.0615\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 269: val_loss improved from 0.06147 to 0.06135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0415 - val_loss: 0.0613\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 270: val_loss improved from 0.06135 to 0.06123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0413 - val_loss: 0.0612\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 271: val_loss improved from 0.06123 to 0.06111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0412 - val_loss: 0.0611\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 272: val_loss improved from 0.06111 to 0.06100, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0410 - val_loss: 0.0610\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 273: val_loss improved from 0.06100 to 0.06088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0408 - val_loss: 0.0609\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 274: val_loss improved from 0.06088 to 0.06077, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0406 - val_loss: 0.0608\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 275: val_loss improved from 0.06077 to 0.06066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0405 - val_loss: 0.0607\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 276: val_loss improved from 0.06066 to 0.06056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0403 - val_loss: 0.0606\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 277: val_loss improved from 0.06056 to 0.06045, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0402 - val_loss: 0.0604\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 278: val_loss improved from 0.06045 to 0.06035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0400 - val_loss: 0.0603\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 279: val_loss improved from 0.06035 to 0.06024, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0398 - val_loss: 0.0602\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 280: val_loss improved from 0.06024 to 0.06014, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0397 - val_loss: 0.0601\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 281: val_loss improved from 0.06014 to 0.06004, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0395 - val_loss: 0.0600\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 282: val_loss improved from 0.06004 to 0.05995, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0394 - val_loss: 0.0599\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 283: val_loss improved from 0.05995 to 0.05985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0392 - val_loss: 0.0598\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 284: val_loss improved from 0.05985 to 0.05976, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0391 - val_loss: 0.0598\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 285: val_loss improved from 0.05976 to 0.05966, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0389 - val_loss: 0.0597\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 286: val_loss improved from 0.05966 to 0.05957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0388 - val_loss: 0.0596\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 287: val_loss improved from 0.05957 to 0.05948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0387 - val_loss: 0.0595\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 288: val_loss improved from 0.05948 to 0.05939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0385 - val_loss: 0.0594\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 289: val_loss improved from 0.05939 to 0.05931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0384 - val_loss: 0.0593\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 290: val_loss improved from 0.05931 to 0.05922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0383 - val_loss: 0.0592\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 291: val_loss improved from 0.05922 to 0.05914, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0381 - val_loss: 0.0591\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 292: val_loss improved from 0.05914 to 0.05905, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0380 - val_loss: 0.0591\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 293: val_loss improved from 0.05905 to 0.05897, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0379 - val_loss: 0.0590\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 294: val_loss improved from 0.05897 to 0.05889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0377 - val_loss: 0.0589\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 295: val_loss improved from 0.05889 to 0.05881, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 296: val_loss improved from 0.05881 to 0.05874, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0375 - val_loss: 0.0587\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 297: val_loss improved from 0.05874 to 0.05866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0373 - val_loss: 0.0587\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 298: val_loss improved from 0.05866 to 0.05859, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0372 - val_loss: 0.0586\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 299: val_loss improved from 0.05859 to 0.05851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0371 - val_loss: 0.0585\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 300: val_loss improved from 0.05851 to 0.05844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0370 - val_loss: 0.0584\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 301: val_loss improved from 0.05844 to 0.05837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 302: val_loss improved from 0.05837 to 0.05830, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0368 - val_loss: 0.0583\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 303: val_loss improved from 0.05830 to 0.05823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0366 - val_loss: 0.0582\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 304: val_loss improved from 0.05823 to 0.05816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 305: val_loss improved from 0.05816 to 0.05810, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 306: val_loss improved from 0.05810 to 0.05803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0363 - val_loss: 0.0580\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 307: val_loss improved from 0.05803 to 0.05797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0362 - val_loss: 0.0580\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 308: val_loss improved from 0.05797 to 0.05791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0361 - val_loss: 0.0579\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 309: val_loss improved from 0.05791 to 0.05785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0360 - val_loss: 0.0578\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 310: val_loss improved from 0.05785 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 311: val_loss improved from 0.05778 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0358 - val_loss: 0.0577\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 312: val_loss improved from 0.05773 to 0.05767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 313: val_loss improved from 0.05767 to 0.05761, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 314: val_loss improved from 0.05761 to 0.05755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 315: val_loss improved from 0.05755 to 0.05750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 316: val_loss improved from 0.05750 to 0.05744, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0353 - val_loss: 0.0574\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 317: val_loss improved from 0.05744 to 0.05739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 318: val_loss improved from 0.05739 to 0.05734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0351 - val_loss: 0.0573\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 319: val_loss improved from 0.05734 to 0.05729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 320: val_loss improved from 0.05729 to 0.05724, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0349 - val_loss: 0.0572\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 321: val_loss improved from 0.05724 to 0.05719, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 322: val_loss improved from 0.05719 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 323: val_loss improved from 0.05714 to 0.05709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 324: val_loss improved from 0.05709 to 0.05704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0345 - val_loss: 0.0570\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 325: val_loss improved from 0.05704 to 0.05700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0345 - val_loss: 0.0570\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 326: val_loss improved from 0.05700 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 327: val_loss improved from 0.05695 to 0.05691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 328: val_loss improved from 0.05691 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0342 - val_loss: 0.0569\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 329: val_loss improved from 0.05687 to 0.05682, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 330: val_loss improved from 0.05682 to 0.05678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 331: val_loss improved from 0.05678 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0340 - val_loss: 0.0567\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 332: val_loss improved from 0.05674 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 333: val_loss improved from 0.05670 to 0.05666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 334: val_loss improved from 0.05666 to 0.05662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 335: val_loss improved from 0.05662 to 0.05658, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 336: val_loss improved from 0.05658 to 0.05655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 337: val_loss improved from 0.05655 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 338: val_loss improved from 0.05651 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 339: val_loss improved from 0.05648 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0334 - val_loss: 0.0564\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 340: val_loss improved from 0.05644 to 0.05641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 341: val_loss improved from 0.05641 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 342: val_loss improved from 0.05637 to 0.05634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0332 - val_loss: 0.0563\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 343: val_loss improved from 0.05634 to 0.05631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 344: val_loss improved from 0.05631 to 0.05628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 345: val_loss improved from 0.05628 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0330 - val_loss: 0.0562\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 346: val_loss improved from 0.05625 to 0.05622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 347: val_loss improved from 0.05622 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 348: val_loss improved from 0.05619 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 349: val_loss improved from 0.05616 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 350: val_loss improved from 0.05613 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 351: val_loss improved from 0.05610 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 352: val_loss improved from 0.05608 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 353: val_loss improved from 0.05605 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 354: val_loss improved from 0.05602 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 355: val_loss improved from 0.05600 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 356: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0323 - val_loss: 0.0559\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 357: val_loss improved from 0.05595 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 358: val_loss improved from 0.05593 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 359: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 360: val_loss improved from 0.05588 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 361: val_loss improved from 0.05586 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 362: val_loss improved from 0.05584 to 0.05581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 363: val_loss improved from 0.05581 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 364: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 365: val_loss improved from 0.05577 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 366: val_loss improved from 0.05575 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 367: val_loss improved from 0.05574 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 368: val_loss improved from 0.05572 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 369: val_loss improved from 0.05570 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 370: val_loss improved from 0.05568 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 371: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 372: val_loss improved from 0.05565 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 373: val_loss improved from 0.05563 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 374: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 375: val_loss improved from 0.05560 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 376: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 377: val_loss improved from 0.05557 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 378: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 379: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 380: val_loss improved from 0.05553 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 381: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 382: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 383: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 384: val_loss improved from 0.05548 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 385: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 386: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 387: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 388: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 389: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 390: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 391: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 392: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 393/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 393: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 394/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 394: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 395/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 395: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 396/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 396: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 397/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 397: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 398/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 398: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 399/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 399: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 400/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 400: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 401/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 401: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 402/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 402: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 403/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 403: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 404/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 404: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 405/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 405: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 406/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 406: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 407/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 407: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 408/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 408: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 409/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 409: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 410/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 410: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 411/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 411: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 412/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 412: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 413/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 413: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 414/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 414: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 415/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 415: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 416/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 416: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 417/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 417: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 418/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 418: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 419/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 419: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 420/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 420: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 421/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 421: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 422/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 422: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 423/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 423: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 424/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 424: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 425/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 425: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 426/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 426: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 427/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 427: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 428/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 428: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 429/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 429: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 430/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 430: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 431/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 431: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 432/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 432: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 433/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 433: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 434/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 434: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 435/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 435: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 436/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 436: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 437/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 437: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 438/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 438: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 439/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 439: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 440/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 440: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 441/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 441: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 442/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 442: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 443/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 443: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 444/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 444: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 445/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 445: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 446/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 446: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 447/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 447: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 448/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 448: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 449/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 449: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 450/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 450: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 451/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 451: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 452/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 452: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 453/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 453: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 454/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 454: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 455/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 455: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 456/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 456: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 457/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 457: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 458/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 458: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 459/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 459: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 460/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 460: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 461/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 461: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 462/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 462: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 463/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 463: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 464/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 464: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 465/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 465: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 466/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 466: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 467/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 467: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 468/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 468: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 469/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 469: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 470/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 470: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 471/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 471: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 472/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 472: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 473/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 473: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 474/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 474: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 475/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 475: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 476/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 476: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 477/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 477: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 478/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 478: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 479/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 479: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 480/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 480: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 481/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 481: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 482/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 482: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 483/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 483: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 484/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 484: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 485/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 485: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 486/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 486: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 487/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 487: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 488/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 488: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 489/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 489: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 490/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 490: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 491/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 491: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 492/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 492: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 493/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 493: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 494/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 494: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 495/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 495: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 496/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 496: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 497/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 497: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 498/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 498: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 499/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 499: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 500/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 500: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 501/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 501: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 502/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 502: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 503/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 503: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 504/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 504: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 505/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 505: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 506/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 506: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 507/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 507: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 508/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 508: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 509/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 509: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 510/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 510: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 511/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 511: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 512/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 512: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 513/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 513: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 514/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 514: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 515/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 515: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 516/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 516: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 517/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 517: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 518/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 518: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 519/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 519: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 520/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 520: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 521/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 521: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 522/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 522: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 523/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 523: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 524/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 524: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 525/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 525: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 526/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 526: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 527/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 527: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 528/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 528: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 529/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 529: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 530/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 530: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 531/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 531: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 532/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 532: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 533/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 533: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 534/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 534: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 535/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 535: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 536/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 536: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0285 - val_loss: 0.0555\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0733\n","loss_and_metrics : 0.07333411276340485\n","1/1 [==============================] - 0s 101ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","from keras.optimizers import Nadam\n","from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","#model.compile(loss='mean_squared_error', optimizer='adamax')\n","#model.compile(loss='mean_squared_error', optimizer='nadam')\n","#model.compile(loss='mean_squared_error', optimizer='rmsprop')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}