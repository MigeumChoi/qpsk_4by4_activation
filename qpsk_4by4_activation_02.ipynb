{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiAL35lfNJc0rAzY72N3+F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5n-fM-jsjOAC","executionInfo":{"status":"ok","timestamp":1695615484217,"user_tz":-540,"elapsed":38064,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"94afad82-1eca-4b02-ee0c-5a3231772d41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3616\n","Epoch 1: val_loss improved from inf to 0.34536, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3616 - val_loss: 0.3454\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3573\n","Epoch 2: val_loss improved from 0.34536 to 0.34134, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.3573 - val_loss: 0.3413\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3530\n","Epoch 3: val_loss improved from 0.34134 to 0.33736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3530 - val_loss: 0.3374\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3487\n","Epoch 4: val_loss improved from 0.33736 to 0.33344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3487 - val_loss: 0.3334\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3445\n","Epoch 5: val_loss improved from 0.33344 to 0.32957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3445 - val_loss: 0.3296\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3403\n","Epoch 6: val_loss improved from 0.32957 to 0.32575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.3403 - val_loss: 0.3257\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3362\n","Epoch 7: val_loss improved from 0.32575 to 0.32198, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3362 - val_loss: 0.3220\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3322\n","Epoch 8: val_loss improved from 0.32198 to 0.31825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3322 - val_loss: 0.3183\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3282\n","Epoch 9: val_loss improved from 0.31825 to 0.31458, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.3282 - val_loss: 0.3146\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3242\n","Epoch 10: val_loss improved from 0.31458 to 0.31095, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3242 - val_loss: 0.3109\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3203\n","Epoch 11: val_loss improved from 0.31095 to 0.30736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3203 - val_loss: 0.3074\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3165\n","Epoch 12: val_loss improved from 0.30736 to 0.30383, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.3165 - val_loss: 0.3038\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3127\n","Epoch 13: val_loss improved from 0.30383 to 0.30033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.3127 - val_loss: 0.3003\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3089\n","Epoch 14: val_loss improved from 0.30033 to 0.29688, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3089 - val_loss: 0.2969\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3052\n","Epoch 15: val_loss improved from 0.29688 to 0.29348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.3052 - val_loss: 0.2935\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3015\n","Epoch 16: val_loss improved from 0.29348 to 0.29012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3015 - val_loss: 0.2901\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2979\n","Epoch 17: val_loss improved from 0.29012 to 0.28680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2979 - val_loss: 0.2868\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2943\n","Epoch 18: val_loss improved from 0.28680 to 0.28352, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2943 - val_loss: 0.2835\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2907\n","Epoch 19: val_loss improved from 0.28352 to 0.28028, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2907 - val_loss: 0.2803\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2872\n","Epoch 20: val_loss improved from 0.28028 to 0.27708, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2872 - val_loss: 0.2771\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2838\n","Epoch 21: val_loss improved from 0.27708 to 0.27392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2838 - val_loss: 0.2739\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2804\n","Epoch 22: val_loss improved from 0.27392 to 0.27080, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2804 - val_loss: 0.2708\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2770\n","Epoch 23: val_loss improved from 0.27080 to 0.26772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2770 - val_loss: 0.2677\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2737\n","Epoch 24: val_loss improved from 0.26772 to 0.26468, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2737 - val_loss: 0.2647\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2704\n","Epoch 25: val_loss improved from 0.26468 to 0.26168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2704 - val_loss: 0.2617\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2671\n","Epoch 26: val_loss improved from 0.26168 to 0.25871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2671 - val_loss: 0.2587\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2639\n","Epoch 27: val_loss improved from 0.25871 to 0.25578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2639 - val_loss: 0.2558\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2607\n","Epoch 28: val_loss improved from 0.25578 to 0.25289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2607 - val_loss: 0.2529\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2576\n","Epoch 29: val_loss improved from 0.25289 to 0.25003, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2576 - val_loss: 0.2500\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2545\n","Epoch 30: val_loss improved from 0.25003 to 0.24720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2545 - val_loss: 0.2472\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2514\n","Epoch 31: val_loss improved from 0.24720 to 0.24441, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2514 - val_loss: 0.2444\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2484\n","Epoch 32: val_loss improved from 0.24441 to 0.24166, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2484 - val_loss: 0.2417\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2454\n","Epoch 33: val_loss improved from 0.24166 to 0.23894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2454 - val_loss: 0.2389\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2424\n","Epoch 34: val_loss improved from 0.23894 to 0.23625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2424 - val_loss: 0.2363\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2395\n","Epoch 35: val_loss improved from 0.23625 to 0.23360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2395 - val_loss: 0.2336\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2366\n","Epoch 36: val_loss improved from 0.23360 to 0.23098, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2366 - val_loss: 0.2310\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2337\n","Epoch 37: val_loss improved from 0.23098 to 0.22839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2337 - val_loss: 0.2284\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2309\n","Epoch 38: val_loss improved from 0.22839 to 0.22583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2309 - val_loss: 0.2258\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2281\n","Epoch 39: val_loss improved from 0.22583 to 0.22330, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2281 - val_loss: 0.2233\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2253\n","Epoch 40: val_loss improved from 0.22330 to 0.22081, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2253 - val_loss: 0.2208\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2226\n","Epoch 41: val_loss improved from 0.22081 to 0.21834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2226 - val_loss: 0.2183\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2199\n","Epoch 42: val_loss improved from 0.21834 to 0.21591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2199 - val_loss: 0.2159\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2172\n","Epoch 43: val_loss improved from 0.21591 to 0.21351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2172 - val_loss: 0.2135\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2146\n","Epoch 44: val_loss improved from 0.21351 to 0.21113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2146 - val_loss: 0.2111\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2120\n","Epoch 45: val_loss improved from 0.21113 to 0.20879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2120 - val_loss: 0.2088\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 46: val_loss improved from 0.20879 to 0.20647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2094 - val_loss: 0.2065\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2069\n","Epoch 47: val_loss improved from 0.20647 to 0.20418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2069 - val_loss: 0.2042\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2044\n","Epoch 48: val_loss improved from 0.20418 to 0.20192, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.2044 - val_loss: 0.2019\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2019\n","Epoch 49: val_loss improved from 0.20192 to 0.19969, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2019 - val_loss: 0.1997\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1994\n","Epoch 50: val_loss improved from 0.19969 to 0.19749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1994 - val_loss: 0.1975\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1970\n","Epoch 51: val_loss improved from 0.19749 to 0.19531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1970 - val_loss: 0.1953\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1946\n","Epoch 52: val_loss improved from 0.19531 to 0.19316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1946 - val_loss: 0.1932\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1922\n","Epoch 53: val_loss improved from 0.19316 to 0.19103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1922 - val_loss: 0.1910\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 54: val_loss improved from 0.19103 to 0.18894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1899 - val_loss: 0.1889\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1876\n","Epoch 55: val_loss improved from 0.18894 to 0.18687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1876 - val_loss: 0.1869\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1853\n","Epoch 56: val_loss improved from 0.18687 to 0.18482, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1853 - val_loss: 0.1848\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1830\n","Epoch 57: val_loss improved from 0.18482 to 0.18280, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1830 - val_loss: 0.1828\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1808\n","Epoch 58: val_loss improved from 0.18280 to 0.18081, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1808 - val_loss: 0.1808\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1786\n","Epoch 59: val_loss improved from 0.18081 to 0.17884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1786 - val_loss: 0.1788\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1764\n","Epoch 60: val_loss improved from 0.17884 to 0.17689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1764 - val_loss: 0.1769\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1742\n","Epoch 61: val_loss improved from 0.17689 to 0.17497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1742 - val_loss: 0.1750\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1721\n","Epoch 62: val_loss improved from 0.17497 to 0.17307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1721 - val_loss: 0.1731\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1700\n","Epoch 63: val_loss improved from 0.17307 to 0.17120, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1700 - val_loss: 0.1712\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1679\n","Epoch 64: val_loss improved from 0.17120 to 0.16935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1679 - val_loss: 0.1694\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1659\n","Epoch 65: val_loss improved from 0.16935 to 0.16753, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1659 - val_loss: 0.1675\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1638\n","Epoch 66: val_loss improved from 0.16753 to 0.16573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.1638 - val_loss: 0.1657\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1618\n","Epoch 67: val_loss improved from 0.16573 to 0.16395, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1618 - val_loss: 0.1639\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1598\n","Epoch 68: val_loss improved from 0.16395 to 0.16219, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.1598 - val_loss: 0.1622\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1579\n","Epoch 69: val_loss improved from 0.16219 to 0.16045, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1579 - val_loss: 0.1605\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1559\n","Epoch 70: val_loss improved from 0.16045 to 0.15874, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1559 - val_loss: 0.1587\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1540\n","Epoch 71: val_loss improved from 0.15874 to 0.15705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1540 - val_loss: 0.1571\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1521\n","Epoch 72: val_loss improved from 0.15705 to 0.15538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.1521 - val_loss: 0.1554\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1503\n","Epoch 73: val_loss improved from 0.15538 to 0.15374, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1503 - val_loss: 0.1537\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1484\n","Epoch 74: val_loss improved from 0.15374 to 0.15211, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1484 - val_loss: 0.1521\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1466\n","Epoch 75: val_loss improved from 0.15211 to 0.15051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.1466 - val_loss: 0.1505\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1448\n","Epoch 76: val_loss improved from 0.15051 to 0.14892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.1448 - val_loss: 0.1489\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1430\n","Epoch 77: val_loss improved from 0.14892 to 0.14736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.1430 - val_loss: 0.1474\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1413\n","Epoch 78: val_loss improved from 0.14736 to 0.14582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.1413 - val_loss: 0.1458\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1395\n","Epoch 79: val_loss improved from 0.14582 to 0.14429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.1395 - val_loss: 0.1443\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1378\n","Epoch 80: val_loss improved from 0.14429 to 0.14279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.1378 - val_loss: 0.1428\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1361\n","Epoch 81: val_loss improved from 0.14279 to 0.14131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1361 - val_loss: 0.1413\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1344\n","Epoch 82: val_loss improved from 0.14131 to 0.13985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1344 - val_loss: 0.1398\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1328\n","Epoch 83: val_loss improved from 0.13985 to 0.13840, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1328 - val_loss: 0.1384\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1311\n","Epoch 84: val_loss improved from 0.13840 to 0.13698, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.1311 - val_loss: 0.1370\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1295\n","Epoch 85: val_loss improved from 0.13698 to 0.13557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.1295 - val_loss: 0.1356\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1279\n","Epoch 86: val_loss improved from 0.13557 to 0.13419, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.1279 - val_loss: 0.1342\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1264\n","Epoch 87: val_loss improved from 0.13419 to 0.13282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.1264 - val_loss: 0.1328\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1248\n","Epoch 88: val_loss improved from 0.13282 to 0.13147, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1248 - val_loss: 0.1315\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1233\n","Epoch 89: val_loss improved from 0.13147 to 0.13014, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.1233 - val_loss: 0.1301\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1218\n","Epoch 90: val_loss improved from 0.13014 to 0.12883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.1218 - val_loss: 0.1288\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1203\n","Epoch 91: val_loss improved from 0.12883 to 0.12753, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.1203 - val_loss: 0.1275\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1188\n","Epoch 92: val_loss improved from 0.12753 to 0.12626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1188 - val_loss: 0.1263\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1173\n","Epoch 93: val_loss improved from 0.12626 to 0.12500, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1173 - val_loss: 0.1250\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1159\n","Epoch 94: val_loss improved from 0.12500 to 0.12375, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1159 - val_loss: 0.1238\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1145\n","Epoch 95: val_loss improved from 0.12375 to 0.12253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1145 - val_loss: 0.1225\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1131\n","Epoch 96: val_loss improved from 0.12253 to 0.12132, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1131 - val_loss: 0.1213\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1117\n","Epoch 97: val_loss improved from 0.12132 to 0.12013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1117 - val_loss: 0.1201\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1103\n","Epoch 98: val_loss improved from 0.12013 to 0.11896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.1103 - val_loss: 0.1190\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1090\n","Epoch 99: val_loss improved from 0.11896 to 0.11780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1090 - val_loss: 0.1178\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1076\n","Epoch 100: val_loss improved from 0.11780 to 0.11666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1076 - val_loss: 0.1167\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 101: val_loss improved from 0.11666 to 0.11554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1063 - val_loss: 0.1155\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1050\n","Epoch 102: val_loss improved from 0.11554 to 0.11443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1050 - val_loss: 0.1144\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1037\n","Epoch 103: val_loss improved from 0.11443 to 0.11333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1037 - val_loss: 0.1133\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1025\n","Epoch 104: val_loss improved from 0.11333 to 0.11226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1025 - val_loss: 0.1123\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1012\n","Epoch 105: val_loss improved from 0.11226 to 0.11120, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1012 - val_loss: 0.1112\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1000\n","Epoch 106: val_loss improved from 0.11120 to 0.11015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1000 - val_loss: 0.1102\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0988\n","Epoch 107: val_loss improved from 0.11015 to 0.10912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0988 - val_loss: 0.1091\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0976\n","Epoch 108: val_loss improved from 0.10912 to 0.10811, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0976 - val_loss: 0.1081\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0964\n","Epoch 109: val_loss improved from 0.10811 to 0.10710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0964 - val_loss: 0.1071\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0952\n","Epoch 110: val_loss improved from 0.10710 to 0.10612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0952 - val_loss: 0.1061\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0941\n","Epoch 111: val_loss improved from 0.10612 to 0.10515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0941 - val_loss: 0.1051\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0930\n","Epoch 112: val_loss improved from 0.10515 to 0.10419, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0930 - val_loss: 0.1042\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0918\n","Epoch 113: val_loss improved from 0.10419 to 0.10325, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0918 - val_loss: 0.1033\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0907\n","Epoch 114: val_loss improved from 0.10325 to 0.10232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0907 - val_loss: 0.1023\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0897\n","Epoch 115: val_loss improved from 0.10232 to 0.10141, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0897 - val_loss: 0.1014\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0886\n","Epoch 116: val_loss improved from 0.10141 to 0.10051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0886 - val_loss: 0.1005\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0875\n","Epoch 117: val_loss improved from 0.10051 to 0.09962, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0875 - val_loss: 0.0996\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0865\n","Epoch 118: val_loss improved from 0.09962 to 0.09875, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0865 - val_loss: 0.0988\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0855\n","Epoch 119: val_loss improved from 0.09875 to 0.09789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0855 - val_loss: 0.0979\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0844\n","Epoch 120: val_loss improved from 0.09789 to 0.09705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0844 - val_loss: 0.0970\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0834\n","Epoch 121: val_loss improved from 0.09705 to 0.09622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0834 - val_loss: 0.0962\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0825\n","Epoch 122: val_loss improved from 0.09622 to 0.09540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0825 - val_loss: 0.0954\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0815\n","Epoch 123: val_loss improved from 0.09540 to 0.09459, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0815 - val_loss: 0.0946\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0805\n","Epoch 124: val_loss improved from 0.09459 to 0.09380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0805 - val_loss: 0.0938\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0796\n","Epoch 125: val_loss improved from 0.09380 to 0.09302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0796 - val_loss: 0.0930\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0786\n","Epoch 126: val_loss improved from 0.09302 to 0.09225, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0786 - val_loss: 0.0922\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0777\n","Epoch 127: val_loss improved from 0.09225 to 0.09149, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0777 - val_loss: 0.0915\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0768\n","Epoch 128: val_loss improved from 0.09149 to 0.09075, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0768 - val_loss: 0.0908\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0759\n","Epoch 129: val_loss improved from 0.09075 to 0.09002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0759 - val_loss: 0.0900\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0751\n","Epoch 130: val_loss improved from 0.09002 to 0.08930, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0751 - val_loss: 0.0893\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0742\n","Epoch 131: val_loss improved from 0.08930 to 0.08859, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0742 - val_loss: 0.0886\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0733\n","Epoch 132: val_loss improved from 0.08859 to 0.08790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0733 - val_loss: 0.0879\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0725\n","Epoch 133: val_loss improved from 0.08790 to 0.08721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0725 - val_loss: 0.0872\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0717\n","Epoch 134: val_loss improved from 0.08721 to 0.08654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0717 - val_loss: 0.0865\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0709\n","Epoch 135: val_loss improved from 0.08654 to 0.08588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0709 - val_loss: 0.0859\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0701\n","Epoch 136: val_loss improved from 0.08588 to 0.08523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0701 - val_loss: 0.0852\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0693\n","Epoch 137: val_loss improved from 0.08523 to 0.08459, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0693 - val_loss: 0.0846\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0685\n","Epoch 138: val_loss improved from 0.08459 to 0.08396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0685 - val_loss: 0.0840\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0677\n","Epoch 139: val_loss improved from 0.08396 to 0.08335, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0677 - val_loss: 0.0833\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0670\n","Epoch 140: val_loss improved from 0.08335 to 0.08274, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0670 - val_loss: 0.0827\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0662\n","Epoch 141: val_loss improved from 0.08274 to 0.08214, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0662 - val_loss: 0.0821\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0655\n","Epoch 142: val_loss improved from 0.08214 to 0.08156, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0655 - val_loss: 0.0816\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 143: val_loss improved from 0.08156 to 0.08098, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0648 - val_loss: 0.0810\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 144: val_loss improved from 0.08098 to 0.08042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0641 - val_loss: 0.0804\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0634\n","Epoch 145: val_loss improved from 0.08042 to 0.07986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0634 - val_loss: 0.0799\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0627\n","Epoch 146: val_loss improved from 0.07986 to 0.07932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0627 - val_loss: 0.0793\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0620\n","Epoch 147: val_loss improved from 0.07932 to 0.07878, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0620 - val_loss: 0.0788\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0613\n","Epoch 148: val_loss improved from 0.07878 to 0.07826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0613 - val_loss: 0.0783\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0607\n","Epoch 149: val_loss improved from 0.07826 to 0.07774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0607 - val_loss: 0.0777\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 150: val_loss improved from 0.07774 to 0.07724, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0600 - val_loss: 0.0772\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0594\n","Epoch 151: val_loss improved from 0.07724 to 0.07674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0594 - val_loss: 0.0767\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0588\n","Epoch 152: val_loss improved from 0.07674 to 0.07625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0588 - val_loss: 0.0763\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0582\n","Epoch 153: val_loss improved from 0.07625 to 0.07577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0582 - val_loss: 0.0758\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 154: val_loss improved from 0.07577 to 0.07530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0576 - val_loss: 0.0753\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 155: val_loss improved from 0.07530 to 0.07484, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0570 - val_loss: 0.0748\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 156: val_loss improved from 0.07484 to 0.07439, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0564 - val_loss: 0.0744\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 157: val_loss improved from 0.07439 to 0.07395, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0558 - val_loss: 0.0739\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0553\n","Epoch 158: val_loss improved from 0.07395 to 0.07351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0553 - val_loss: 0.0735\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 159: val_loss improved from 0.07351 to 0.07309, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0547 - val_loss: 0.0731\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 160: val_loss improved from 0.07309 to 0.07267, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0542 - val_loss: 0.0727\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 161: val_loss improved from 0.07267 to 0.07226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0536 - val_loss: 0.0723\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0531\n","Epoch 162: val_loss improved from 0.07226 to 0.07186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0531 - val_loss: 0.0719\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0526\n","Epoch 163: val_loss improved from 0.07186 to 0.07146, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0526 - val_loss: 0.0715\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 164: val_loss improved from 0.07146 to 0.07108, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0521 - val_loss: 0.0711\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0516\n","Epoch 165: val_loss improved from 0.07108 to 0.07070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0516 - val_loss: 0.0707\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0511\n","Epoch 166: val_loss improved from 0.07070 to 0.07033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0511 - val_loss: 0.0703\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 167: val_loss improved from 0.07033 to 0.06996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0506 - val_loss: 0.0700\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0501\n","Epoch 168: val_loss improved from 0.06996 to 0.06961, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0501 - val_loss: 0.0696\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0496\n","Epoch 169: val_loss improved from 0.06961 to 0.06926, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0496 - val_loss: 0.0693\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 170: val_loss improved from 0.06926 to 0.06892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0492 - val_loss: 0.0689\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 171: val_loss improved from 0.06892 to 0.06858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0487 - val_loss: 0.0686\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 172: val_loss improved from 0.06858 to 0.06826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0483 - val_loss: 0.0683\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 173: val_loss improved from 0.06826 to 0.06794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0479 - val_loss: 0.0679\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 174: val_loss improved from 0.06794 to 0.06762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0474 - val_loss: 0.0676\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 175: val_loss improved from 0.06762 to 0.06731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0470 - val_loss: 0.0673\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 176: val_loss improved from 0.06731 to 0.06701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0466 - val_loss: 0.0670\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 177: val_loss improved from 0.06701 to 0.06672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0462 - val_loss: 0.0667\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 178: val_loss improved from 0.06672 to 0.06643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0458 - val_loss: 0.0664\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 179: val_loss improved from 0.06643 to 0.06615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0454 - val_loss: 0.0662\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 180: val_loss improved from 0.06615 to 0.06588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0450 - val_loss: 0.0659\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 181: val_loss improved from 0.06588 to 0.06561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0446 - val_loss: 0.0656\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0443\n","Epoch 182: val_loss improved from 0.06561 to 0.06534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0443 - val_loss: 0.0653\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 183: val_loss improved from 0.06534 to 0.06509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0439 - val_loss: 0.0651\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 184: val_loss improved from 0.06509 to 0.06484, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0436 - val_loss: 0.0648\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 185: val_loss improved from 0.06484 to 0.06459, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0432 - val_loss: 0.0646\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 186: val_loss improved from 0.06459 to 0.06435, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0429 - val_loss: 0.0643\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 187: val_loss improved from 0.06435 to 0.06411, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0425 - val_loss: 0.0641\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 188: val_loss improved from 0.06411 to 0.06389, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0422 - val_loss: 0.0639\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 189: val_loss improved from 0.06389 to 0.06366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0419 - val_loss: 0.0637\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 190: val_loss improved from 0.06366 to 0.06344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0416 - val_loss: 0.0634\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 191: val_loss improved from 0.06344 to 0.06323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0413 - val_loss: 0.0632\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 192: val_loss improved from 0.06323 to 0.06302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0410 - val_loss: 0.0630\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 193: val_loss improved from 0.06302 to 0.06282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0407 - val_loss: 0.0628\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 194: val_loss improved from 0.06282 to 0.06262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0404 - val_loss: 0.0626\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 195: val_loss improved from 0.06262 to 0.06242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0401 - val_loss: 0.0624\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 196: val_loss improved from 0.06242 to 0.06224, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0398 - val_loss: 0.0622\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 197: val_loss improved from 0.06224 to 0.06205, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0395 - val_loss: 0.0621\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 198: val_loss improved from 0.06205 to 0.06187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0392 - val_loss: 0.0619\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 199: val_loss improved from 0.06187 to 0.06170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0390 - val_loss: 0.0617\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 200: val_loss improved from 0.06170 to 0.06152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0387 - val_loss: 0.0615\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 201: val_loss improved from 0.06152 to 0.06136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0385 - val_loss: 0.0614\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 202: val_loss improved from 0.06136 to 0.06120, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0382 - val_loss: 0.0612\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 203: val_loss improved from 0.06120 to 0.06104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0380 - val_loss: 0.0610\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 204: val_loss improved from 0.06104 to 0.06088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0377 - val_loss: 0.0609\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 205: val_loss improved from 0.06088 to 0.06073, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0375 - val_loss: 0.0607\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 206: val_loss improved from 0.06073 to 0.06059, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0373 - val_loss: 0.0606\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 207: val_loss improved from 0.06059 to 0.06044, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0370 - val_loss: 0.0604\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 208: val_loss improved from 0.06044 to 0.06030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0368 - val_loss: 0.0603\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 209: val_loss improved from 0.06030 to 0.06017, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0366 - val_loss: 0.0602\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 210: val_loss improved from 0.06017 to 0.06004, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0364 - val_loss: 0.0600\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 211: val_loss improved from 0.06004 to 0.05991, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0362 - val_loss: 0.0599\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 212: val_loss improved from 0.05991 to 0.05978, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0360 - val_loss: 0.0598\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 213: val_loss improved from 0.05978 to 0.05966, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0358 - val_loss: 0.0597\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 214: val_loss improved from 0.05966 to 0.05955, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0356 - val_loss: 0.0595\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 215: val_loss improved from 0.05955 to 0.05943, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0354 - val_loss: 0.0594\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 216: val_loss improved from 0.05943 to 0.05932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0352 - val_loss: 0.0593\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 217: val_loss improved from 0.05932 to 0.05921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0350 - val_loss: 0.0592\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 218: val_loss improved from 0.05921 to 0.05911, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0348 - val_loss: 0.0591\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 219: val_loss improved from 0.05911 to 0.05900, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0347 - val_loss: 0.0590\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 220: val_loss improved from 0.05900 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0345 - val_loss: 0.0589\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 221: val_loss improved from 0.05891 to 0.05881, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0343 - val_loss: 0.0588\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 222: val_loss improved from 0.05881 to 0.05872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0342 - val_loss: 0.0587\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 223: val_loss improved from 0.05872 to 0.05862, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0340 - val_loss: 0.0586\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 224: val_loss improved from 0.05862 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0338 - val_loss: 0.0585\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 225: val_loss improved from 0.05854 to 0.05845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0337 - val_loss: 0.0585\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 226: val_loss improved from 0.05845 to 0.05837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0335 - val_loss: 0.0584\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 227: val_loss improved from 0.05837 to 0.05829, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0334 - val_loss: 0.0583\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 228: val_loss improved from 0.05829 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0332 - val_loss: 0.0582\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 229: val_loss improved from 0.05821 to 0.05814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0331 - val_loss: 0.0581\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 230: val_loss improved from 0.05814 to 0.05806, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0330 - val_loss: 0.0581\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 231: val_loss improved from 0.05806 to 0.05799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0328 - val_loss: 0.0580\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 232: val_loss improved from 0.05799 to 0.05792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.0327 - val_loss: 0.0579\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 233: val_loss improved from 0.05792 to 0.05786, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0326 - val_loss: 0.0579\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 234: val_loss improved from 0.05786 to 0.05779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0324 - val_loss: 0.0578\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 235: val_loss improved from 0.05779 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0323 - val_loss: 0.0577\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 236: val_loss improved from 0.05773 to 0.05767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0322 - val_loss: 0.0577\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 237: val_loss improved from 0.05767 to 0.05761, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0321 - val_loss: 0.0576\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 238: val_loss improved from 0.05761 to 0.05756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0320 - val_loss: 0.0576\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 239: val_loss improved from 0.05756 to 0.05750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0319 - val_loss: 0.0575\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 240: val_loss improved from 0.05750 to 0.05745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0317 - val_loss: 0.0575\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 241: val_loss improved from 0.05745 to 0.05740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0316 - val_loss: 0.0574\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 242: val_loss improved from 0.05740 to 0.05735, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 142ms/step - loss: 0.0315 - val_loss: 0.0574\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 243: val_loss improved from 0.05735 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0314 - val_loss: 0.0573\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 244: val_loss improved from 0.05731 to 0.05726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0313 - val_loss: 0.0573\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 245: val_loss improved from 0.05726 to 0.05722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0312 - val_loss: 0.0572\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 246: val_loss improved from 0.05722 to 0.05718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0311 - val_loss: 0.0572\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 247: val_loss improved from 0.05718 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0310 - val_loss: 0.0571\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 248: val_loss improved from 0.05714 to 0.05710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0309 - val_loss: 0.0571\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 249: val_loss improved from 0.05710 to 0.05706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0309 - val_loss: 0.0571\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 250: val_loss improved from 0.05706 to 0.05702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0308 - val_loss: 0.0570\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 251: val_loss improved from 0.05702 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0307 - val_loss: 0.0570\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 252: val_loss improved from 0.05699 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0306 - val_loss: 0.0570\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 253: val_loss improved from 0.05696 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0305 - val_loss: 0.0569\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 254: val_loss improved from 0.05693 to 0.05690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0304 - val_loss: 0.0569\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 255: val_loss improved from 0.05690 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0304 - val_loss: 0.0569\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 256: val_loss improved from 0.05687 to 0.05684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0303 - val_loss: 0.0568\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 257: val_loss improved from 0.05684 to 0.05681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0302 - val_loss: 0.0568\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 258: val_loss improved from 0.05681 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0301 - val_loss: 0.0568\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 259: val_loss improved from 0.05679 to 0.05676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0301 - val_loss: 0.0568\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 260: val_loss improved from 0.05676 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0300 - val_loss: 0.0567\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 261: val_loss improved from 0.05674 to 0.05672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0299 - val_loss: 0.0567\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 262: val_loss improved from 0.05672 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0299 - val_loss: 0.0567\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 263: val_loss improved from 0.05670 to 0.05668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0298 - val_loss: 0.0567\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 264: val_loss improved from 0.05668 to 0.05666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0297 - val_loss: 0.0567\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 265: val_loss improved from 0.05666 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0297 - val_loss: 0.0566\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 266: val_loss improved from 0.05664 to 0.05662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0296 - val_loss: 0.0566\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 267: val_loss improved from 0.05662 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0295 - val_loss: 0.0566\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 268: val_loss improved from 0.05661 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0295 - val_loss: 0.0566\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 269: val_loss improved from 0.05659 to 0.05658, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0294 - val_loss: 0.0566\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 270: val_loss improved from 0.05658 to 0.05656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0294 - val_loss: 0.0566\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 271: val_loss improved from 0.05656 to 0.05655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0293 - val_loss: 0.0566\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 272: val_loss improved from 0.05655 to 0.05654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0293 - val_loss: 0.0565\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 273: val_loss improved from 0.05654 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0292 - val_loss: 0.0565\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 274: val_loss improved from 0.05653 to 0.05652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0292 - val_loss: 0.0565\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 275: val_loss improved from 0.05652 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0291 - val_loss: 0.0565\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 276: val_loss improved from 0.05651 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0291 - val_loss: 0.0565\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 277: val_loss improved from 0.05650 to 0.05649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0290 - val_loss: 0.0565\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 278: val_loss improved from 0.05649 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0290 - val_loss: 0.0565\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 279: val_loss improved from 0.05648 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0289 - val_loss: 0.0565\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 280: val_loss improved from 0.05648 to 0.05647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0289 - val_loss: 0.0565\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 281: val_loss improved from 0.05647 to 0.05647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0289 - val_loss: 0.0565\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 282: val_loss improved from 0.05647 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0288 - val_loss: 0.0565\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 283: val_loss improved from 0.05646 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0288 - val_loss: 0.0565\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 284: val_loss improved from 0.05646 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0287 - val_loss: 0.0565\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 285: val_loss improved from 0.05645 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0287 - val_loss: 0.0564\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 286: val_loss improved from 0.05645 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0287 - val_loss: 0.0564\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 287: val_loss improved from 0.05645 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0286 - val_loss: 0.0564\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 288: val_loss improved from 0.05644 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0286 - val_loss: 0.0564\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 289: val_loss improved from 0.05644 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0285 - val_loss: 0.0564\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 290: val_loss improved from 0.05644 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0285 - val_loss: 0.0564\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 291: val_loss improved from 0.05644 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0285 - val_loss: 0.0564\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 292: val_loss improved from 0.05644 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0285 - val_loss: 0.0564\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 293: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0284 - val_loss: 0.0564\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 294: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0284 - val_loss: 0.0564\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 295: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0284 - val_loss: 0.0564\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 296: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0283 - val_loss: 0.0564\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 297: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0283 - val_loss: 0.0564\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 298: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0283 - val_loss: 0.0564\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 299: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0282 - val_loss: 0.0564\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 300: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0282 - val_loss: 0.0564\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 301: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0282 - val_loss: 0.0565\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 302: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0282 - val_loss: 0.0565\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 303: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0281 - val_loss: 0.0565\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 304: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0281 - val_loss: 0.0565\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 305: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0281 - val_loss: 0.0565\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 306: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0281 - val_loss: 0.0565\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 307: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 308: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 309: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 310: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 311: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 312: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 313: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 314: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 315: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 316: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 317: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0278 - val_loss: 0.0565\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 318: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0278 - val_loss: 0.0565\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 319: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0278 - val_loss: 0.0565\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 320: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 321: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 322: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 323: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 324: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 325: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 326: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 327: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 328: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 329: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 330: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0277 - val_loss: 0.0566\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 331: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0276 - val_loss: 0.0566\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 332: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0276 - val_loss: 0.0566\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 333: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0276 - val_loss: 0.0566\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 334: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0276 - val_loss: 0.0567\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 335: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0276 - val_loss: 0.0567\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 336: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0276 - val_loss: 0.0567\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 337: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0276 - val_loss: 0.0567\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 338: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0276 - val_loss: 0.0567\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 339: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 340: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 341: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 342: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 343: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 344: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 345: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0275 - val_loss: 0.0567\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 346: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0275 - val_loss: 0.0568\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 347: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0275 - val_loss: 0.0568\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 348: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0275 - val_loss: 0.0568\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 349: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0275 - val_loss: 0.0568\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 350: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 351: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 352: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 353: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 354: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 355: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 356: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 357: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 358: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0274 - val_loss: 0.0568\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 359: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0274 - val_loss: 0.0569\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 360: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0274 - val_loss: 0.0569\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 361: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0274 - val_loss: 0.0569\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 362: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0274 - val_loss: 0.0569\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 363: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 364: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 365: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 366: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 367: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 368: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 369: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 370: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0273 - val_loss: 0.0569\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 371: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 372: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 373: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 374: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 375: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 376: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 377: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 378: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 379: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 380: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 381: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0273 - val_loss: 0.0570\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 382: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0272 - val_loss: 0.0570\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 383: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0272 - val_loss: 0.0570\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 384: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 385: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 386: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 387: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 388: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 389: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 390: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 391: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0272 - val_loss: 0.0571\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 392: val_loss did not improve from 0.05644\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0272 - val_loss: 0.0571\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0733\n","loss_and_metrics : 0.07325546443462372\n","1/1 [==============================] - 0s 180ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","from keras.optimizers import Nadam\n","from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","#model.compile(loss='mean_squared_error', optimizer='adamax')\n","#model.compile(loss='mean_squared_error', optimizer='nadam')\n","#model.compile(loss='mean_squared_error', optimizer='rmsprop')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}