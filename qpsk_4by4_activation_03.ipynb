{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCFjUu1Azfr+D99RgJL5Yt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UJxvwXtfzTSy","executionInfo":{"status":"ok","timestamp":1695619863335,"user_tz":-540,"elapsed":59067,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"18d9acc3-65fd-41b0-d63c-59b826461ae1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3644\n","Epoch 1: val_loss improved from inf to 0.34622, saving model to hl5_0100.h5\n","1/1 [==============================] - 2s 2s/step - loss: 0.3644 - val_loss: 0.3462\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3597\n","Epoch 2: val_loss improved from 0.34622 to 0.34187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.3597 - val_loss: 0.3419\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3551\n","Epoch 3: val_loss improved from 0.34187 to 0.33759, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 190ms/step - loss: 0.3551 - val_loss: 0.3376\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3505\n","Epoch 4: val_loss improved from 0.33759 to 0.33336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.3505 - val_loss: 0.3334\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3459\n","Epoch 5: val_loss improved from 0.33336 to 0.32919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 189ms/step - loss: 0.3459 - val_loss: 0.3292\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3415\n","Epoch 6: val_loss improved from 0.32919 to 0.32508, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 160ms/step - loss: 0.3415 - val_loss: 0.3251\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3371\n","Epoch 7: val_loss improved from 0.32508 to 0.32103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 184ms/step - loss: 0.3371 - val_loss: 0.3210\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3327\n","Epoch 8: val_loss improved from 0.32103 to 0.31704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 195ms/step - loss: 0.3327 - val_loss: 0.3170\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3284\n","Epoch 9: val_loss improved from 0.31704 to 0.31310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 182ms/step - loss: 0.3284 - val_loss: 0.3131\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3242\n","Epoch 10: val_loss improved from 0.31310 to 0.30921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 220ms/step - loss: 0.3242 - val_loss: 0.3092\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3200\n","Epoch 11: val_loss improved from 0.30921 to 0.30538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.3200 - val_loss: 0.3054\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3159\n","Epoch 12: val_loss improved from 0.30538 to 0.30160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.3159 - val_loss: 0.3016\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3118\n","Epoch 13: val_loss improved from 0.30160 to 0.29787, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.3118 - val_loss: 0.2979\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3078\n","Epoch 14: val_loss improved from 0.29787 to 0.29420, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 207ms/step - loss: 0.3078 - val_loss: 0.2942\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3039\n","Epoch 15: val_loss improved from 0.29420 to 0.29057, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.3039 - val_loss: 0.2906\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2999\n","Epoch 16: val_loss improved from 0.29057 to 0.28700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 174ms/step - loss: 0.2999 - val_loss: 0.2870\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2961\n","Epoch 17: val_loss improved from 0.28700 to 0.28347, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.2961 - val_loss: 0.2835\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2923\n","Epoch 18: val_loss improved from 0.28347 to 0.27999, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.2923 - val_loss: 0.2800\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2885\n","Epoch 19: val_loss improved from 0.27999 to 0.27656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 144ms/step - loss: 0.2885 - val_loss: 0.2766\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2848\n","Epoch 20: val_loss improved from 0.27656 to 0.27317, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.2848 - val_loss: 0.2732\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2812\n","Epoch 21: val_loss improved from 0.27317 to 0.26983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 170ms/step - loss: 0.2812 - val_loss: 0.2698\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2775\n","Epoch 22: val_loss improved from 0.26983 to 0.26653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.2775 - val_loss: 0.2665\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2740\n","Epoch 23: val_loss improved from 0.26653 to 0.26328, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.2740 - val_loss: 0.2633\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2705\n","Epoch 24: val_loss improved from 0.26328 to 0.26007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.2705 - val_loss: 0.2601\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2670\n","Epoch 25: val_loss improved from 0.26007 to 0.25690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.2670 - val_loss: 0.2569\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2635\n","Epoch 26: val_loss improved from 0.25690 to 0.25378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 163ms/step - loss: 0.2635 - val_loss: 0.2538\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2602\n","Epoch 27: val_loss improved from 0.25378 to 0.25070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 146ms/step - loss: 0.2602 - val_loss: 0.2507\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2568\n","Epoch 28: val_loss improved from 0.25070 to 0.24765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 288ms/step - loss: 0.2568 - val_loss: 0.2477\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2535\n","Epoch 29: val_loss improved from 0.24765 to 0.24465, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 306ms/step - loss: 0.2535 - val_loss: 0.2447\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2502\n","Epoch 30: val_loss improved from 0.24465 to 0.24169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 264ms/step - loss: 0.2502 - val_loss: 0.2417\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2470\n","Epoch 31: val_loss improved from 0.24169 to 0.23877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 271ms/step - loss: 0.2470 - val_loss: 0.2388\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2438\n","Epoch 32: val_loss improved from 0.23877 to 0.23588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 223ms/step - loss: 0.2438 - val_loss: 0.2359\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2407\n","Epoch 33: val_loss improved from 0.23588 to 0.23304, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 191ms/step - loss: 0.2407 - val_loss: 0.2330\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2376\n","Epoch 34: val_loss improved from 0.23304 to 0.23023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 203ms/step - loss: 0.2376 - val_loss: 0.2302\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2345\n","Epoch 35: val_loss improved from 0.23023 to 0.22746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 187ms/step - loss: 0.2345 - val_loss: 0.2275\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2315\n","Epoch 36: val_loss improved from 0.22746 to 0.22472, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 311ms/step - loss: 0.2315 - val_loss: 0.2247\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2285\n","Epoch 37: val_loss improved from 0.22472 to 0.22202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 400ms/step - loss: 0.2285 - val_loss: 0.2220\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2256\n","Epoch 38: val_loss improved from 0.22202 to 0.21936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 453ms/step - loss: 0.2256 - val_loss: 0.2194\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2227\n","Epoch 39: val_loss improved from 0.21936 to 0.21673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 268ms/step - loss: 0.2227 - val_loss: 0.2167\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2198\n","Epoch 40: val_loss improved from 0.21673 to 0.21414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 203ms/step - loss: 0.2198 - val_loss: 0.2141\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2169\n","Epoch 41: val_loss improved from 0.21414 to 0.21158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.2169 - val_loss: 0.2116\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2141\n","Epoch 42: val_loss improved from 0.21158 to 0.20905, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.2141 - val_loss: 0.2091\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2114\n","Epoch 43: val_loss improved from 0.20905 to 0.20656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.2114 - val_loss: 0.2066\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 44: val_loss improved from 0.20656 to 0.20410, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.2086 - val_loss: 0.2041\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2059\n","Epoch 45: val_loss improved from 0.20410 to 0.20167, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 167ms/step - loss: 0.2059 - val_loss: 0.2017\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2033\n","Epoch 46: val_loss improved from 0.20167 to 0.19928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 220ms/step - loss: 0.2033 - val_loss: 0.1993\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2006\n","Epoch 47: val_loss improved from 0.19928 to 0.19691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 173ms/step - loss: 0.2006 - val_loss: 0.1969\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1980\n","Epoch 48: val_loss improved from 0.19691 to 0.19458, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1980 - val_loss: 0.1946\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1955\n","Epoch 49: val_loss improved from 0.19458 to 0.19228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1955 - val_loss: 0.1923\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1929\n","Epoch 50: val_loss improved from 0.19228 to 0.19001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1929 - val_loss: 0.1900\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1904\n","Epoch 51: val_loss improved from 0.19001 to 0.18777, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1904 - val_loss: 0.1878\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1879\n","Epoch 52: val_loss improved from 0.18777 to 0.18556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1879 - val_loss: 0.1856\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1855\n","Epoch 53: val_loss improved from 0.18556 to 0.18338, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1855 - val_loss: 0.1834\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1831\n","Epoch 54: val_loss improved from 0.18338 to 0.18123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1831 - val_loss: 0.1812\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1807\n","Epoch 55: val_loss improved from 0.18123 to 0.17910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1807 - val_loss: 0.1791\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1784\n","Epoch 56: val_loss improved from 0.17910 to 0.17701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1784 - val_loss: 0.1770\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1760\n","Epoch 57: val_loss improved from 0.17701 to 0.17494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1760 - val_loss: 0.1749\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1737\n","Epoch 58: val_loss improved from 0.17494 to 0.17290, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1737 - val_loss: 0.1729\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1715\n","Epoch 59: val_loss improved from 0.17290 to 0.17089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1715 - val_loss: 0.1709\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1692\n","Epoch 60: val_loss improved from 0.17089 to 0.16891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1692 - val_loss: 0.1689\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1670\n","Epoch 61: val_loss improved from 0.16891 to 0.16695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1670 - val_loss: 0.1669\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1648\n","Epoch 62: val_loss improved from 0.16695 to 0.16502, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1648 - val_loss: 0.1650\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1627\n","Epoch 63: val_loss improved from 0.16502 to 0.16311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1627 - val_loss: 0.1631\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1606\n","Epoch 64: val_loss improved from 0.16311 to 0.16124, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1606 - val_loss: 0.1612\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1585\n","Epoch 65: val_loss improved from 0.16124 to 0.15938, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1585 - val_loss: 0.1594\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1564\n","Epoch 66: val_loss improved from 0.15938 to 0.15756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1564 - val_loss: 0.1576\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1544\n","Epoch 67: val_loss improved from 0.15756 to 0.15575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1544 - val_loss: 0.1558\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1523\n","Epoch 68: val_loss improved from 0.15575 to 0.15398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1523 - val_loss: 0.1540\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1503\n","Epoch 69: val_loss improved from 0.15398 to 0.15222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1503 - val_loss: 0.1522\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1484\n","Epoch 70: val_loss improved from 0.15222 to 0.15050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1484 - val_loss: 0.1505\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1464\n","Epoch 71: val_loss improved from 0.15050 to 0.14879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1464 - val_loss: 0.1488\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1445\n","Epoch 72: val_loss improved from 0.14879 to 0.14711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1445 - val_loss: 0.1471\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1426\n","Epoch 73: val_loss improved from 0.14711 to 0.14545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1426 - val_loss: 0.1455\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1408\n","Epoch 74: val_loss improved from 0.14545 to 0.14382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1408 - val_loss: 0.1438\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1389\n","Epoch 75: val_loss improved from 0.14382 to 0.14221, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1389 - val_loss: 0.1422\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1371\n","Epoch 76: val_loss improved from 0.14221 to 0.14062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1371 - val_loss: 0.1406\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1353\n","Epoch 77: val_loss improved from 0.14062 to 0.13906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1353 - val_loss: 0.1391\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1335\n","Epoch 78: val_loss improved from 0.13906 to 0.13751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1335 - val_loss: 0.1375\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1318\n","Epoch 79: val_loss improved from 0.13751 to 0.13599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1318 - val_loss: 0.1360\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1301\n","Epoch 80: val_loss improved from 0.13599 to 0.13449, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1301 - val_loss: 0.1345\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1283\n","Epoch 81: val_loss improved from 0.13449 to 0.13302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1283 - val_loss: 0.1330\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1267\n","Epoch 82: val_loss improved from 0.13302 to 0.13156, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1267 - val_loss: 0.1316\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1250\n","Epoch 83: val_loss improved from 0.13156 to 0.13013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1250 - val_loss: 0.1301\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1234\n","Epoch 84: val_loss improved from 0.13013 to 0.12871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1234 - val_loss: 0.1287\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1218\n","Epoch 85: val_loss improved from 0.12871 to 0.12732, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1218 - val_loss: 0.1273\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1202\n","Epoch 86: val_loss improved from 0.12732 to 0.12595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1202 - val_loss: 0.1259\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1186\n","Epoch 87: val_loss improved from 0.12595 to 0.12460, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1186 - val_loss: 0.1246\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1170\n","Epoch 88: val_loss improved from 0.12460 to 0.12326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1170 - val_loss: 0.1233\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1155\n","Epoch 89: val_loss improved from 0.12326 to 0.12195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1155 - val_loss: 0.1220\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1140\n","Epoch 90: val_loss improved from 0.12195 to 0.12066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1140 - val_loss: 0.1207\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1125\n","Epoch 91: val_loss improved from 0.12066 to 0.11939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1125 - val_loss: 0.1194\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1111\n","Epoch 92: val_loss improved from 0.11939 to 0.11813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1111 - val_loss: 0.1181\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1096\n","Epoch 93: val_loss improved from 0.11813 to 0.11690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1096 - val_loss: 0.1169\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1082\n","Epoch 94: val_loss improved from 0.11690 to 0.11568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1082 - val_loss: 0.1157\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1068\n","Epoch 95: val_loss improved from 0.11568 to 0.11449, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1068 - val_loss: 0.1145\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1054\n","Epoch 96: val_loss improved from 0.11449 to 0.11331, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1054 - val_loss: 0.1133\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1040\n","Epoch 97: val_loss improved from 0.11331 to 0.11215, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1040 - val_loss: 0.1121\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1027\n","Epoch 98: val_loss improved from 0.11215 to 0.11101, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1027 - val_loss: 0.1110\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1014\n","Epoch 99: val_loss improved from 0.11101 to 0.10988, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1014 - val_loss: 0.1099\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1000\n","Epoch 100: val_loss improved from 0.10988 to 0.10878, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1000 - val_loss: 0.1088\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0988\n","Epoch 101: val_loss improved from 0.10878 to 0.10769, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0988 - val_loss: 0.1077\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 102: val_loss improved from 0.10769 to 0.10662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0975 - val_loss: 0.1066\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0962\n","Epoch 103: val_loss improved from 0.10662 to 0.10556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0962 - val_loss: 0.1056\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0950\n","Epoch 104: val_loss improved from 0.10556 to 0.10452, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0950 - val_loss: 0.1045\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0938\n","Epoch 105: val_loss improved from 0.10452 to 0.10350, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0938 - val_loss: 0.1035\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0926\n","Epoch 106: val_loss improved from 0.10350 to 0.10250, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0926 - val_loss: 0.1025\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0914\n","Epoch 107: val_loss improved from 0.10250 to 0.10151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0914 - val_loss: 0.1015\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0902\n","Epoch 108: val_loss improved from 0.10151 to 0.10054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0902 - val_loss: 0.1005\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0891\n","Epoch 109: val_loss improved from 0.10054 to 0.09959, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0891 - val_loss: 0.0996\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0879\n","Epoch 110: val_loss improved from 0.09959 to 0.09865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0879 - val_loss: 0.0986\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0868\n","Epoch 111: val_loss improved from 0.09865 to 0.09772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0868 - val_loss: 0.0977\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0857\n","Epoch 112: val_loss improved from 0.09772 to 0.09681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0857 - val_loss: 0.0968\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0846\n","Epoch 113: val_loss improved from 0.09681 to 0.09592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0846 - val_loss: 0.0959\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0836\n","Epoch 114: val_loss improved from 0.09592 to 0.09504, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0836 - val_loss: 0.0950\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0825\n","Epoch 115: val_loss improved from 0.09504 to 0.09418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0825 - val_loss: 0.0942\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0815\n","Epoch 116: val_loss improved from 0.09418 to 0.09333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0815 - val_loss: 0.0933\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0805\n","Epoch 117: val_loss improved from 0.09333 to 0.09250, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0805 - val_loss: 0.0925\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0795\n","Epoch 118: val_loss improved from 0.09250 to 0.09168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0795 - val_loss: 0.0917\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0785\n","Epoch 119: val_loss improved from 0.09168 to 0.09088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0785 - val_loss: 0.0909\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0775\n","Epoch 120: val_loss improved from 0.09088 to 0.09009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0775 - val_loss: 0.0901\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 121: val_loss improved from 0.09009 to 0.08931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0766 - val_loss: 0.0893\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0756\n","Epoch 122: val_loss improved from 0.08931 to 0.08855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0756 - val_loss: 0.0885\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0747\n","Epoch 123: val_loss improved from 0.08855 to 0.08780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0747 - val_loss: 0.0878\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0738\n","Epoch 124: val_loss improved from 0.08780 to 0.08706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0738 - val_loss: 0.0871\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0729\n","Epoch 125: val_loss improved from 0.08706 to 0.08634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0729 - val_loss: 0.0863\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0720\n","Epoch 126: val_loss improved from 0.08634 to 0.08563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0720 - val_loss: 0.0856\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0711\n","Epoch 127: val_loss improved from 0.08563 to 0.08494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0711 - val_loss: 0.0849\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0703\n","Epoch 128: val_loss improved from 0.08494 to 0.08425, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0703 - val_loss: 0.0843\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0694\n","Epoch 129: val_loss improved from 0.08425 to 0.08358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0694 - val_loss: 0.0836\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0686\n","Epoch 130: val_loss improved from 0.08358 to 0.08293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0686 - val_loss: 0.0829\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0678\n","Epoch 131: val_loss improved from 0.08293 to 0.08228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0678 - val_loss: 0.0823\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0670\n","Epoch 132: val_loss improved from 0.08228 to 0.08165, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0670 - val_loss: 0.0816\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0662\n","Epoch 133: val_loss improved from 0.08165 to 0.08103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0662 - val_loss: 0.0810\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0654\n","Epoch 134: val_loss improved from 0.08103 to 0.08042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0654 - val_loss: 0.0804\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0647\n","Epoch 135: val_loss improved from 0.08042 to 0.07982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0647 - val_loss: 0.0798\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0639\n","Epoch 136: val_loss improved from 0.07982 to 0.07923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0639 - val_loss: 0.0792\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0632\n","Epoch 137: val_loss improved from 0.07923 to 0.07866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0632 - val_loss: 0.0787\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0625\n","Epoch 138: val_loss improved from 0.07866 to 0.07809, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0625 - val_loss: 0.0781\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0618\n","Epoch 139: val_loss improved from 0.07809 to 0.07754, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0618 - val_loss: 0.0775\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0611\n","Epoch 140: val_loss improved from 0.07754 to 0.07700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0611 - val_loss: 0.0770\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0604\n","Epoch 141: val_loss improved from 0.07700 to 0.07647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0604 - val_loss: 0.0765\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0597\n","Epoch 142: val_loss improved from 0.07647 to 0.07595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0597 - val_loss: 0.0760\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0590\n","Epoch 143: val_loss improved from 0.07595 to 0.07544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0590 - val_loss: 0.0754\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0584\n","Epoch 144: val_loss improved from 0.07544 to 0.07494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0584 - val_loss: 0.0749\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 145: val_loss improved from 0.07494 to 0.07445, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0577 - val_loss: 0.0745\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 146: val_loss improved from 0.07445 to 0.07397, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0571 - val_loss: 0.0740\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 147: val_loss improved from 0.07397 to 0.07351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0565 - val_loss: 0.0735\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 148: val_loss improved from 0.07351 to 0.07305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0559 - val_loss: 0.0730\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0553\n","Epoch 149: val_loss improved from 0.07305 to 0.07260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0553 - val_loss: 0.0726\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 150: val_loss improved from 0.07260 to 0.07216, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0547 - val_loss: 0.0722\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0541\n","Epoch 151: val_loss improved from 0.07216 to 0.07173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0541 - val_loss: 0.0717\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0535\n","Epoch 152: val_loss improved from 0.07173 to 0.07131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0535 - val_loss: 0.0713\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 153: val_loss improved from 0.07131 to 0.07089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0530 - val_loss: 0.0709\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 154: val_loss improved from 0.07089 to 0.07049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0524 - val_loss: 0.0705\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 155: val_loss improved from 0.07049 to 0.07010, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0519 - val_loss: 0.0701\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 156: val_loss improved from 0.07010 to 0.06971, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0514 - val_loss: 0.0697\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 157: val_loss improved from 0.06971 to 0.06933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0509 - val_loss: 0.0693\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 158: val_loss improved from 0.06933 to 0.06897, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0504 - val_loss: 0.0690\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0499\n","Epoch 159: val_loss improved from 0.06897 to 0.06861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0499 - val_loss: 0.0686\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 160: val_loss improved from 0.06861 to 0.06825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0494 - val_loss: 0.0683\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 161: val_loss improved from 0.06825 to 0.06791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0489 - val_loss: 0.0679\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 162: val_loss improved from 0.06791 to 0.06757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0484 - val_loss: 0.0676\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0480\n","Epoch 163: val_loss improved from 0.06757 to 0.06724, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0480 - val_loss: 0.0672\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0475\n","Epoch 164: val_loss improved from 0.06724 to 0.06692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0475 - val_loss: 0.0669\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 165: val_loss improved from 0.06692 to 0.06661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0471 - val_loss: 0.0666\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 166: val_loss improved from 0.06661 to 0.06630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0466 - val_loss: 0.0663\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 167: val_loss improved from 0.06630 to 0.06601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0462 - val_loss: 0.0660\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 168: val_loss improved from 0.06601 to 0.06571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0458 - val_loss: 0.0657\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 169: val_loss improved from 0.06571 to 0.06543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0454 - val_loss: 0.0654\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 170: val_loss improved from 0.06543 to 0.06515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0450 - val_loss: 0.0652\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 171: val_loss improved from 0.06515 to 0.06488, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0446 - val_loss: 0.0649\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 172: val_loss improved from 0.06488 to 0.06462, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0442 - val_loss: 0.0646\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 173: val_loss improved from 0.06462 to 0.06436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0438 - val_loss: 0.0644\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 174: val_loss improved from 0.06436 to 0.06411, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0434 - val_loss: 0.0641\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 175: val_loss improved from 0.06411 to 0.06386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0431 - val_loss: 0.0639\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 176: val_loss improved from 0.06386 to 0.06362, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0427 - val_loss: 0.0636\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 177: val_loss improved from 0.06362 to 0.06339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0424 - val_loss: 0.0634\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 178: val_loss improved from 0.06339 to 0.06316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0420 - val_loss: 0.0632\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 179: val_loss improved from 0.06316 to 0.06294, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0417 - val_loss: 0.0629\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 180: val_loss improved from 0.06294 to 0.06273, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0414 - val_loss: 0.0627\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 181: val_loss improved from 0.06273 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0410 - val_loss: 0.0625\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 182: val_loss improved from 0.06252 to 0.06232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0407 - val_loss: 0.0623\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 183: val_loss improved from 0.06232 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0404 - val_loss: 0.0621\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 184: val_loss improved from 0.06212 to 0.06192, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0401 - val_loss: 0.0619\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 185: val_loss improved from 0.06192 to 0.06174, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0398 - val_loss: 0.0617\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 186: val_loss improved from 0.06174 to 0.06155, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0395 - val_loss: 0.0616\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 187: val_loss improved from 0.06155 to 0.06138, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0392 - val_loss: 0.0614\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 188: val_loss improved from 0.06138 to 0.06120, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0390 - val_loss: 0.0612\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 189: val_loss improved from 0.06120 to 0.06103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0387 - val_loss: 0.0610\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 190: val_loss improved from 0.06103 to 0.06087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0384 - val_loss: 0.0609\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 191: val_loss improved from 0.06087 to 0.06071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0382 - val_loss: 0.0607\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 192: val_loss improved from 0.06071 to 0.06056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0379 - val_loss: 0.0606\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 193: val_loss improved from 0.06056 to 0.06041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0377 - val_loss: 0.0604\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 194: val_loss improved from 0.06041 to 0.06026, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0374 - val_loss: 0.0603\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 195: val_loss improved from 0.06026 to 0.06012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0372 - val_loss: 0.0601\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 196: val_loss improved from 0.06012 to 0.05998, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0369 - val_loss: 0.0600\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 197: val_loss improved from 0.05998 to 0.05985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0367 - val_loss: 0.0599\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 198: val_loss improved from 0.05985 to 0.05972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0365 - val_loss: 0.0597\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 199: val_loss improved from 0.05972 to 0.05960, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0363 - val_loss: 0.0596\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 200: val_loss improved from 0.05960 to 0.05948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0361 - val_loss: 0.0595\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 201: val_loss improved from 0.05948 to 0.05936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0358 - val_loss: 0.0594\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 202: val_loss improved from 0.05936 to 0.05925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0356 - val_loss: 0.0592\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 203: val_loss improved from 0.05925 to 0.05914, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0354 - val_loss: 0.0591\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 204: val_loss improved from 0.05914 to 0.05903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0352 - val_loss: 0.0590\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 205: val_loss improved from 0.05903 to 0.05893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0350 - val_loss: 0.0589\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 206: val_loss improved from 0.05893 to 0.05883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0349 - val_loss: 0.0588\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 207: val_loss improved from 0.05883 to 0.05873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0347 - val_loss: 0.0587\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 208: val_loss improved from 0.05873 to 0.05864, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0345 - val_loss: 0.0586\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 209: val_loss improved from 0.05864 to 0.05855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0343 - val_loss: 0.0585\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 210: val_loss improved from 0.05855 to 0.05846, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0341 - val_loss: 0.0585\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 211: val_loss improved from 0.05846 to 0.05837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0340 - val_loss: 0.0584\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 212: val_loss improved from 0.05837 to 0.05829, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0338 - val_loss: 0.0583\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 213: val_loss improved from 0.05829 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0337 - val_loss: 0.0582\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 214: val_loss improved from 0.05821 to 0.05814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0335 - val_loss: 0.0581\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 215: val_loss improved from 0.05814 to 0.05807, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0333 - val_loss: 0.0581\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 216: val_loss improved from 0.05807 to 0.05799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0332 - val_loss: 0.0580\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 217: val_loss improved from 0.05799 to 0.05793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0330 - val_loss: 0.0579\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 218: val_loss improved from 0.05793 to 0.05786, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0329 - val_loss: 0.0579\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 219: val_loss improved from 0.05786 to 0.05780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0328 - val_loss: 0.0578\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 220: val_loss improved from 0.05780 to 0.05774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0326 - val_loss: 0.0577\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 221: val_loss improved from 0.05774 to 0.05768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0325 - val_loss: 0.0577\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 222: val_loss improved from 0.05768 to 0.05762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0324 - val_loss: 0.0576\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 223: val_loss improved from 0.05762 to 0.05757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0322 - val_loss: 0.0576\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 224: val_loss improved from 0.05757 to 0.05752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0321 - val_loss: 0.0575\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 225: val_loss improved from 0.05752 to 0.05747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0320 - val_loss: 0.0575\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 226: val_loss improved from 0.05747 to 0.05742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0319 - val_loss: 0.0574\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 227: val_loss improved from 0.05742 to 0.05738, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0318 - val_loss: 0.0574\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 228: val_loss improved from 0.05738 to 0.05733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0316 - val_loss: 0.0573\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 229: val_loss improved from 0.05733 to 0.05729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0315 - val_loss: 0.0573\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 230: val_loss improved from 0.05729 to 0.05725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0314 - val_loss: 0.0573\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 231: val_loss improved from 0.05725 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0313 - val_loss: 0.0572\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 232: val_loss improved from 0.05721 to 0.05718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0312 - val_loss: 0.0572\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 233: val_loss improved from 0.05718 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0311 - val_loss: 0.0571\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 234: val_loss improved from 0.05714 to 0.05711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0310 - val_loss: 0.0571\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 235: val_loss improved from 0.05711 to 0.05708, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0309 - val_loss: 0.0571\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 236: val_loss improved from 0.05708 to 0.05705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0308 - val_loss: 0.0570\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 237: val_loss improved from 0.05705 to 0.05702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0307 - val_loss: 0.0570\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 238: val_loss improved from 0.05702 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0306 - val_loss: 0.0570\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 239: val_loss improved from 0.05699 to 0.05697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0306 - val_loss: 0.0570\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 240: val_loss improved from 0.05697 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0305 - val_loss: 0.0569\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 241: val_loss improved from 0.05695 to 0.05692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0304 - val_loss: 0.0569\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 242: val_loss improved from 0.05692 to 0.05690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0303 - val_loss: 0.0569\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 243: val_loss improved from 0.05690 to 0.05688, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0302 - val_loss: 0.0569\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 244: val_loss improved from 0.05688 to 0.05686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0302 - val_loss: 0.0569\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 245: val_loss improved from 0.05686 to 0.05684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0301 - val_loss: 0.0568\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 246: val_loss improved from 0.05684 to 0.05683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0300 - val_loss: 0.0568\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 247: val_loss improved from 0.05683 to 0.05681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0299 - val_loss: 0.0568\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 248: val_loss improved from 0.05681 to 0.05680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0299 - val_loss: 0.0568\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 249: val_loss improved from 0.05680 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0298 - val_loss: 0.0568\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 250: val_loss improved from 0.05679 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0297 - val_loss: 0.0568\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 251: val_loss improved from 0.05677 to 0.05676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0297 - val_loss: 0.0568\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 252: val_loss improved from 0.05676 to 0.05675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0296 - val_loss: 0.0568\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 253: val_loss improved from 0.05675 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0295 - val_loss: 0.0567\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 254: val_loss improved from 0.05674 to 0.05673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0295 - val_loss: 0.0567\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 255: val_loss improved from 0.05673 to 0.05673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0294 - val_loss: 0.0567\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 256: val_loss improved from 0.05673 to 0.05672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0294 - val_loss: 0.0567\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 257: val_loss improved from 0.05672 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0293 - val_loss: 0.0567\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 258: val_loss improved from 0.05671 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0293 - val_loss: 0.0567\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 259: val_loss improved from 0.05671 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0292 - val_loss: 0.0567\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 260: val_loss improved from 0.05671 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0292 - val_loss: 0.0567\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 261: val_loss improved from 0.05670 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0291 - val_loss: 0.0567\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 262: val_loss improved from 0.05670 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0291 - val_loss: 0.0567\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 263: val_loss improved from 0.05670 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0290 - val_loss: 0.0567\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 264: val_loss improved from 0.05669 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0290 - val_loss: 0.0567\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 265: val_loss improved from 0.05669 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0289 - val_loss: 0.0567\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 266: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0567\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 267: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0567\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 268: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0567\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 269: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0287 - val_loss: 0.0567\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 270: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0287 - val_loss: 0.0567\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 271: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0287 - val_loss: 0.0567\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 272: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0286 - val_loss: 0.0567\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 273: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0286 - val_loss: 0.0567\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 274: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0286 - val_loss: 0.0567\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 275: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0285 - val_loss: 0.0567\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 276: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0285 - val_loss: 0.0567\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 277: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0284 - val_loss: 0.0567\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 278: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0284 - val_loss: 0.0567\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 279: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0284 - val_loss: 0.0567\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 280: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0284 - val_loss: 0.0567\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 281: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0283 - val_loss: 0.0568\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 282: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0283 - val_loss: 0.0568\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 283: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0283 - val_loss: 0.0568\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 284: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0282 - val_loss: 0.0568\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 285: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0282 - val_loss: 0.0568\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 286: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0282 - val_loss: 0.0568\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 287: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0282 - val_loss: 0.0568\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 288: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0281 - val_loss: 0.0568\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 289: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0281 - val_loss: 0.0568\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 290: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0281 - val_loss: 0.0568\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 291: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0281 - val_loss: 0.0568\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 292: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0280 - val_loss: 0.0568\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 293: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0280 - val_loss: 0.0569\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 294: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0280 - val_loss: 0.0569\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 295: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0280 - val_loss: 0.0569\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 296: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 297: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 298: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 299: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 300: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 301: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0279 - val_loss: 0.0569\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 302: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 303: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 304: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 305: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 306: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 307: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0278 - val_loss: 0.0570\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 308: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0277 - val_loss: 0.0570\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 309: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0277 - val_loss: 0.0570\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 310: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0277 - val_loss: 0.0570\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 311: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0277 - val_loss: 0.0571\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 312: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0277 - val_loss: 0.0571\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 313: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0277 - val_loss: 0.0571\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 314: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0277 - val_loss: 0.0571\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 315: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0276 - val_loss: 0.0571\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 316: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0276 - val_loss: 0.0571\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 317: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0276 - val_loss: 0.0571\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 318: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0276 - val_loss: 0.0571\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 319: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0276 - val_loss: 0.0571\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 320: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0276 - val_loss: 0.0572\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 321: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0276 - val_loss: 0.0572\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 322: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0276 - val_loss: 0.0572\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 323: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0275 - val_loss: 0.0572\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 324: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0275 - val_loss: 0.0572\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 325: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0275 - val_loss: 0.0572\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 326: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0275 - val_loss: 0.0572\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 327: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0275 - val_loss: 0.0572\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 328: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 329: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 330: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 331: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 332: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 333: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0275 - val_loss: 0.0573\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 334: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0274 - val_loss: 0.0573\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 335: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0274 - val_loss: 0.0573\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 336: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0274 - val_loss: 0.0573\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 337: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 338: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 339: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 340: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 341: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 342: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 343: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 344: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 345: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0274 - val_loss: 0.0574\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 346: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0274 - val_loss: 0.0575\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 347: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0274 - val_loss: 0.0575\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 348: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 349: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 350: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 351: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 352: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 353: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 354: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 355: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0273 - val_loss: 0.0575\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 356: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 357: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 358: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 359: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 360: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 361: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 362: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 363: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 364: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0273 - val_loss: 0.0576\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 365: val_loss did not improve from 0.05669\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0273 - val_loss: 0.0576\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0744\n","loss_and_metrics : 0.07444611936807632\n","1/1 [==============================] - 0s 108ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","from keras.optimizers import Nadam\n","from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","#model.compile(loss='mean_squared_error', optimizer='adamax')\n","#model.compile(loss='mean_squared_error', optimizer='nadam')\n","#model.compile(loss='mean_squared_error', optimizer='rmsprop')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}